{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323a4995-14a3-4666-a69e-b8446472f606",
   "metadata": {},
   "source": [
    "## Downloading Widerface Dataset from Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f35e9c-5205-487a-9e7a-00603e01c14c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-12T04:49:39.654067Z",
     "start_time": "2024-03-12T04:49:39.647869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\Oak\\\\Documents\\\\git_projects\\\\fd_widerface_yolov8\\\\src'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T04:49:42.205784Z",
     "start_time": "2024-03-12T04:49:42.203613Z"
    }
   },
   "id": "b7016622566bd937",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645098df-dcad-403b-b1df-bd45d6ea724d",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-12T01:54:11.424299Z",
     "start_time": "2024-03-12T01:52:02.678541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.16, to fix: `pip install ultralytics==8.0.196`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in WIDER-FACE-1 to yolov8:: 100%|██████████| 968279/968279 [01:45<00:00, 9190.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to WIDER-FACE-1 in yolov8:: 100%|██████████| 32215/32215 [00:16<00:00, 1963.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"GXx1mInWvTfDrKB73mbr\")\n",
    "project = rf.workspace(\"large-benchmark-datasets\").project(\"wider-face-ndtcz\")\n",
    "dataset = project.version(1).download(\"yolov8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83331c64-d4ab-4a8d-bf53-53af0fe2a0e4",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-12T01:55:47.583071Z",
     "start_time": "2024-03-12T01:55:45.142324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-summary\n",
      "  Downloading torch_summary-1.4.5-py3-none-any.whl.metadata (18 kB)\n",
      "Using cached torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: torch-summary\n",
      "Successfully installed torch-summary-1.4.5\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c1e26cb-10b5-4145-a240-9e559702af09",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-12T01:55:56.456367Z",
     "start_time": "2024-03-12T01:55:54.133945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.23M/6.23M [00:00<00:00, 9.67MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred 355/355 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "(225, 3157200, 3157184, 8.8575488)\n"
     ]
    }
   ],
   "source": [
    "# We will use yolov8n because it is smallest model size, and should be sufficient for non-complex task such as this.\n",
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8n.yaml').load('yolov8n.pt')\n",
    "\n",
    "print(model.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3158a7-45cf-4bc2-92a1-77ddeb83c141",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-12T01:59:48.535613Z",
     "start_time": "2024-03-12T01:59:48.530843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.model.0.conv.weight\n",
      "model.model.0.bn.weight\n",
      "model.model.0.bn.bias\n",
      "model.model.1.conv.weight\n",
      "model.model.1.bn.weight\n",
      "model.model.1.bn.bias\n",
      "model.model.2.cv1.conv.weight\n",
      "model.model.2.cv1.bn.weight\n",
      "model.model.2.cv1.bn.bias\n",
      "model.model.2.cv2.conv.weight\n",
      "model.model.2.cv2.bn.weight\n",
      "model.model.2.cv2.bn.bias\n",
      "model.model.2.m.0.cv1.conv.weight\n",
      "model.model.2.m.0.cv1.bn.weight\n",
      "model.model.2.m.0.cv1.bn.bias\n",
      "model.model.2.m.0.cv2.conv.weight\n",
      "model.model.2.m.0.cv2.bn.weight\n",
      "model.model.2.m.0.cv2.bn.bias\n",
      "model.model.3.conv.weight\n",
      "model.model.3.bn.weight\n",
      "model.model.3.bn.bias\n",
      "model.model.4.cv1.conv.weight\n",
      "model.model.4.cv1.bn.weight\n",
      "model.model.4.cv1.bn.bias\n",
      "model.model.4.cv2.conv.weight\n",
      "model.model.4.cv2.bn.weight\n",
      "model.model.4.cv2.bn.bias\n",
      "model.model.4.m.0.cv1.conv.weight\n",
      "model.model.4.m.0.cv1.bn.weight\n",
      "model.model.4.m.0.cv1.bn.bias\n",
      "model.model.4.m.0.cv2.conv.weight\n",
      "model.model.4.m.0.cv2.bn.weight\n",
      "model.model.4.m.0.cv2.bn.bias\n",
      "model.model.4.m.1.cv1.conv.weight\n",
      "model.model.4.m.1.cv1.bn.weight\n",
      "model.model.4.m.1.cv1.bn.bias\n",
      "model.model.4.m.1.cv2.conv.weight\n",
      "model.model.4.m.1.cv2.bn.weight\n",
      "model.model.4.m.1.cv2.bn.bias\n",
      "model.model.5.conv.weight\n",
      "model.model.5.bn.weight\n",
      "model.model.5.bn.bias\n",
      "model.model.6.cv1.conv.weight\n",
      "model.model.6.cv1.bn.weight\n",
      "model.model.6.cv1.bn.bias\n",
      "model.model.6.cv2.conv.weight\n",
      "model.model.6.cv2.bn.weight\n",
      "model.model.6.cv2.bn.bias\n",
      "model.model.6.m.0.cv1.conv.weight\n",
      "model.model.6.m.0.cv1.bn.weight\n",
      "model.model.6.m.0.cv1.bn.bias\n",
      "model.model.6.m.0.cv2.conv.weight\n",
      "model.model.6.m.0.cv2.bn.weight\n",
      "model.model.6.m.0.cv2.bn.bias\n",
      "model.model.6.m.1.cv1.conv.weight\n",
      "model.model.6.m.1.cv1.bn.weight\n",
      "model.model.6.m.1.cv1.bn.bias\n",
      "model.model.6.m.1.cv2.conv.weight\n",
      "model.model.6.m.1.cv2.bn.weight\n",
      "model.model.6.m.1.cv2.bn.bias\n",
      "model.model.7.conv.weight\n",
      "model.model.7.bn.weight\n",
      "model.model.7.bn.bias\n",
      "model.model.8.cv1.conv.weight\n",
      "model.model.8.cv1.bn.weight\n",
      "model.model.8.cv1.bn.bias\n",
      "model.model.8.cv2.conv.weight\n",
      "model.model.8.cv2.bn.weight\n",
      "model.model.8.cv2.bn.bias\n",
      "model.model.8.m.0.cv1.conv.weight\n",
      "model.model.8.m.0.cv1.bn.weight\n",
      "model.model.8.m.0.cv1.bn.bias\n",
      "model.model.8.m.0.cv2.conv.weight\n",
      "model.model.8.m.0.cv2.bn.weight\n",
      "model.model.8.m.0.cv2.bn.bias\n",
      "model.model.9.cv1.conv.weight\n",
      "model.model.9.cv1.bn.weight\n",
      "model.model.9.cv1.bn.bias\n",
      "model.model.9.cv2.conv.weight\n",
      "model.model.9.cv2.bn.weight\n",
      "model.model.9.cv2.bn.bias\n",
      "model.model.12.cv1.conv.weight\n",
      "model.model.12.cv1.bn.weight\n",
      "model.model.12.cv1.bn.bias\n",
      "model.model.12.cv2.conv.weight\n",
      "model.model.12.cv2.bn.weight\n",
      "model.model.12.cv2.bn.bias\n",
      "model.model.12.m.0.cv1.conv.weight\n",
      "model.model.12.m.0.cv1.bn.weight\n",
      "model.model.12.m.0.cv1.bn.bias\n",
      "model.model.12.m.0.cv2.conv.weight\n",
      "model.model.12.m.0.cv2.bn.weight\n",
      "model.model.12.m.0.cv2.bn.bias\n",
      "model.model.15.cv1.conv.weight\n",
      "model.model.15.cv1.bn.weight\n",
      "model.model.15.cv1.bn.bias\n",
      "model.model.15.cv2.conv.weight\n",
      "model.model.15.cv2.bn.weight\n",
      "model.model.15.cv2.bn.bias\n",
      "model.model.15.m.0.cv1.conv.weight\n",
      "model.model.15.m.0.cv1.bn.weight\n",
      "model.model.15.m.0.cv1.bn.bias\n",
      "model.model.15.m.0.cv2.conv.weight\n",
      "model.model.15.m.0.cv2.bn.weight\n",
      "model.model.15.m.0.cv2.bn.bias\n",
      "model.model.16.conv.weight\n",
      "model.model.16.bn.weight\n",
      "model.model.16.bn.bias\n",
      "model.model.18.cv1.conv.weight\n",
      "model.model.18.cv1.bn.weight\n",
      "model.model.18.cv1.bn.bias\n",
      "model.model.18.cv2.conv.weight\n",
      "model.model.18.cv2.bn.weight\n",
      "model.model.18.cv2.bn.bias\n",
      "model.model.18.m.0.cv1.conv.weight\n",
      "model.model.18.m.0.cv1.bn.weight\n",
      "model.model.18.m.0.cv1.bn.bias\n",
      "model.model.18.m.0.cv2.conv.weight\n",
      "model.model.18.m.0.cv2.bn.weight\n",
      "model.model.18.m.0.cv2.bn.bias\n",
      "model.model.19.conv.weight\n",
      "model.model.19.bn.weight\n",
      "model.model.19.bn.bias\n",
      "model.model.21.cv1.conv.weight\n",
      "model.model.21.cv1.bn.weight\n",
      "model.model.21.cv1.bn.bias\n",
      "model.model.21.cv2.conv.weight\n",
      "model.model.21.cv2.bn.weight\n",
      "model.model.21.cv2.bn.bias\n",
      "model.model.21.m.0.cv1.conv.weight\n",
      "model.model.21.m.0.cv1.bn.weight\n",
      "model.model.21.m.0.cv1.bn.bias\n",
      "model.model.21.m.0.cv2.conv.weight\n",
      "model.model.21.m.0.cv2.bn.weight\n",
      "model.model.21.m.0.cv2.bn.bias\n",
      "model.model.22.cv2.0.0.conv.weight\n",
      "model.model.22.cv2.0.0.bn.weight\n",
      "model.model.22.cv2.0.0.bn.bias\n",
      "model.model.22.cv2.0.1.conv.weight\n",
      "model.model.22.cv2.0.1.bn.weight\n",
      "model.model.22.cv2.0.1.bn.bias\n",
      "model.model.22.cv2.0.2.weight\n",
      "model.model.22.cv2.0.2.bias\n",
      "model.model.22.cv2.1.0.conv.weight\n",
      "model.model.22.cv2.1.0.bn.weight\n",
      "model.model.22.cv2.1.0.bn.bias\n",
      "model.model.22.cv2.1.1.conv.weight\n",
      "model.model.22.cv2.1.1.bn.weight\n",
      "model.model.22.cv2.1.1.bn.bias\n",
      "model.model.22.cv2.1.2.weight\n",
      "model.model.22.cv2.1.2.bias\n",
      "model.model.22.cv2.2.0.conv.weight\n",
      "model.model.22.cv2.2.0.bn.weight\n",
      "model.model.22.cv2.2.0.bn.bias\n",
      "model.model.22.cv2.2.1.conv.weight\n",
      "model.model.22.cv2.2.1.bn.weight\n",
      "model.model.22.cv2.2.1.bn.bias\n",
      "model.model.22.cv2.2.2.weight\n",
      "model.model.22.cv2.2.2.bias\n",
      "model.model.22.cv3.0.0.conv.weight\n",
      "model.model.22.cv3.0.0.bn.weight\n",
      "model.model.22.cv3.0.0.bn.bias\n",
      "model.model.22.cv3.0.1.conv.weight\n",
      "model.model.22.cv3.0.1.bn.weight\n",
      "model.model.22.cv3.0.1.bn.bias\n",
      "model.model.22.cv3.0.2.weight\n",
      "model.model.22.cv3.0.2.bias\n",
      "model.model.22.cv3.1.0.conv.weight\n",
      "model.model.22.cv3.1.0.bn.weight\n",
      "model.model.22.cv3.1.0.bn.bias\n",
      "model.model.22.cv3.1.1.conv.weight\n",
      "model.model.22.cv3.1.1.bn.weight\n",
      "model.model.22.cv3.1.1.bn.bias\n",
      "model.model.22.cv3.1.2.weight\n",
      "model.model.22.cv3.1.2.bias\n",
      "model.model.22.cv3.2.0.conv.weight\n",
      "model.model.22.cv3.2.0.bn.weight\n",
      "model.model.22.cv3.2.0.bn.bias\n",
      "model.model.22.cv3.2.1.conv.weight\n",
      "model.model.22.cv3.2.1.bn.weight\n",
      "model.model.22.cv3.2.1.bn.bias\n",
      "model.model.22.cv3.2.2.weight\n",
      "model.model.22.cv3.2.2.bias\n",
      "model.model.22.dfl.conv.weight\n"
     ]
    }
   ],
   "source": [
    "# Check all the layers to identify if we should be freezing any of the layers during transfer learning\n",
    "for k, v in model.named_parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc07d23b-5be7-4640-82a0-47d1665b9369",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-12T01:59:51.797312Z",
     "start_time": "2024-03-12T01:59:51.714826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "206"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear memory\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n",
      "Number of GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if cuda_available:\n",
    "    print(\"CUDA is available.\")\n",
    "    # You can also print additional information about the available GPUs\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. The code will run on CPU.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T02:00:31.883651Z",
     "start_time": "2024-03-12T02:00:27.422230Z"
    }
   },
   "id": "2b6c63888a87e7b7",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3c60e-7658-4286-8f12-ab495cc382d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 🚀 Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "\u001B[34m\u001B[1mengine/trainer: \u001B[0mtask=detect, mode=train, model=yolov8n.pt, data=data.yaml, epochs=20, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=5, cache=False, device=None, workers=8, project=None, name=train_face23, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train_face23\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "WARNING ⚠️ Comet installed but not initialized correctly, not logging this run. Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY \n",
      "\u001B[34m\u001B[1mTensorBoard: \u001B[0mStart with 'tensorboard --logdir runs/detect/train_face23', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/train/labels.cache... 12877 images, 4 backgrounds, 0 corrupt: 100%|██████████| 12877/12877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/train/images/2_Demonstration_Protesters_2_231_jpg.rf.14b6ba970f3a34843cf41706cf802e37.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/train/images/37_Soccer_Soccer_37_851_jpg.rf.9626888f7c524f523f123df5c7417573.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/train/images/7_Cheering_Cheering_7_17_jpg.rf.f33362e0b02002a17ce25e9fcc6c6650.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/valid/labels.cache... 3226 images, 4 backgrounds, 0 corrupt: 100%|██████████| 3226/3226 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mWARNING ⚠️ /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/valid/images/21_Festival_Festival_21_604_jpg.rf.f65a6018a5c9c5f0a3f93bbe84b46203.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train_face23/labels.jpg... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001B[34m\u001B[1mTensorBoard: \u001B[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001B[1mruns/detect/train_face23\u001B[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      13.8G      1.646     0.9648      1.088         86        640: 100%|██████████| 805/805 [04:06<00:00,  3.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:33<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.782      0.492      0.562       0.29\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      11.9G      1.651     0.9748      1.096        216        640: 100%|██████████| 805/805 [04:26<00:00,  3.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.772      0.479      0.554      0.284\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      12.8G      1.657     0.9923      1.096        103        640: 100%|██████████| 805/805 [04:37<00:00,  2.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:36<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696       0.78      0.478      0.545      0.277\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      14.3G      1.659     0.9902      1.101        205        640: 100%|██████████| 805/805 [04:12<00:00,  3.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.786      0.483      0.554      0.291\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      12.1G      1.651      0.962      1.093         96        640: 100%|██████████| 805/805 [04:22<00:00,  3.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.791      0.505      0.575      0.297\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      12.1G      1.619     0.9387      1.085        119        640: 100%|██████████| 805/805 [03:59<00:00,  3.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:33<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696        0.8      0.509      0.584      0.301\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20      12.3G      1.608     0.9219       1.08        305        640: 100%|██████████| 805/805 [03:59<00:00,  3.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.813      0.512       0.59      0.308\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20      11.2G       1.61     0.9148      1.073        503        640: 100%|██████████| 805/805 [04:01<00:00,  3.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:32<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.808      0.523      0.597      0.314\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20      8.78G      1.579     0.8925      1.073        172        640: 100%|██████████| 805/805 [04:01<00:00,  3.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:36<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.808      0.525      0.602      0.315\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20      9.21G       1.57     0.8754      1.067        116        640: 100%|██████████| 805/805 [04:07<00:00,  3.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.812      0.528      0.608      0.317\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20      10.9G      1.548     0.8477      1.073         87        640: 100%|██████████| 805/805 [04:10<00:00,  3.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:36<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.808      0.523      0.597      0.313\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20      11.7G      1.526     0.8271      1.068         46        640: 100%|██████████| 805/805 [04:34<00:00,  2.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:35<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.818       0.53      0.608      0.318\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      12.1G      1.516       0.81      1.061        100        640: 100%|██████████| 805/805 [04:28<00:00,  3.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:35<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.813      0.535      0.611      0.325\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20      10.3G      1.504     0.7961      1.054        373        640: 100%|██████████| 805/805 [04:22<00:00,  3.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:35<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.825      0.539      0.619       0.33\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20      12.1G      1.494     0.7855      1.054        163        640: 100%|██████████| 805/805 [04:32<00:00,  2.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.825      0.544      0.626      0.334\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      13.5G      1.481     0.7738      1.047        109        640: 100%|██████████| 805/805 [04:20<00:00,  3.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.824       0.55      0.628      0.334\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      13.9G       1.47     0.7619      1.046        118        640: 100%|██████████| 805/805 [04:36<00:00,  2.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.826      0.552      0.631       0.34\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      12.3G      1.456     0.7489       1.04         78        640: 100%|██████████| 805/805 [04:30<00:00,  2.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:35<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696       0.83      0.552      0.634      0.342\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20      13.1G      1.449     0.7364      1.038         64        640: 100%|██████████| 805/805 [04:43<00:00,  2.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:35<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696       0.83      0.554      0.636      0.343\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20        13G      1.432     0.7233      1.032        133        640: 100%|██████████| 805/805 [04:17<00:00,  3.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:35<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696       0.83      0.558      0.638      0.343\n",
      "\n",
      "20 epochs completed in 1.654 hours.\n",
      "Optimizer stripped from runs/detect/train_face23/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train_face23/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train_face23/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.11 🚀 Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   3%|▎         | 3/101 [00:02<01:47,  1.09s/it]"
     ]
    }
   ],
   "source": [
    "results = model.train(data='data.yaml', epochs=20, imgsz=640, batch=16, name=\"train_face2\", verbose=True, plots=True, save_period=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0664e979-004b-4c99-8147-90dbfc289f99",
   "metadata": {},
   "source": [
    "Note that some of the layers within the model has been fused to optimize model performance:\n",
    "\n",
    "https://docs.ultralytics.com/reference/engine/model/#ultralytics.engine.model.Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6a382-4e28-4353-9170-66960e742bbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f0c789-3872-48d2-8761-f7faa43324bc",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-12T04:58:06.893769Z",
     "start_time": "2024-03-12T04:57:12.924809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.16 ðŸš€ Python-3.11.8 torch-2.2.0 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to 'C:\\Users\\Oak\\AppData\\Roaming\\Ultralytics\\Arial.ttf'...\n",
      "\u001B[34m\u001B[1mval: \u001B[0mWARNING âš ï¸� C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\images\\21_Festival_Festival_21_604_jpg.rf.f65a6018a5c9c5f0a3f93bbe84b46203.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mval: \u001B[0mNew cache created: C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels.cache\n",
      "                   all       3226      39696       0.83       0.56      0.639      0.344\n",
      "Speed: 0.2ms preprocess, 2.4ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001B[1mruns\\detect\\val5\u001B[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/755k [00:00<?, ?B/s]\n",
      " 24%|##4       | 184k/755k [00:00<00:00, 1.82MB/s]\n",
      " 60%|######    | 456k/755k [00:00<00:00, 2.31MB/s]\n",
      "100%|##########| 755k/755k [00:00<00:00, 3.12MB/s]\n",
      "\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels...:   0%|          | 0/3226 [00:00<?, ?it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 118 images, 1 backgrounds, 0 corrupt:   4%|3         | 118/3226 [00:00<00:02, 1166.45it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 248 images, 1 backgrounds, 0 corrupt:   8%|7         | 248/3226 [00:00<00:02, 1243.73it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 383 images, 1 backgrounds, 0 corrupt:  12%|#1        | 383/3226 [00:00<00:02, 1282.99it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 519 images, 1 backgrounds, 0 corrupt:  16%|#6        | 519/3226 [00:00<00:02, 1309.17it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 655 images, 1 backgrounds, 0 corrupt:  20%|##        | 655/3226 [00:00<00:01, 1324.57it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 792 images, 1 backgrounds, 0 corrupt:  25%|##4       | 792/3226 [00:00<00:01, 1339.55it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 926 images, 1 backgrounds, 0 corrupt:  29%|##8       | 926/3226 [00:00<00:01, 1326.30it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 1060 images, 1 backgrounds, 0 corrupt:  33%|###2      | 1060/3226 [00:00<00:01, 1323.06it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 1194 images, 1 backgrounds, 0 corrupt:  37%|###7      | 1194/3226 [00:00<00:01, 1321.62it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 1330 images, 1 backgrounds, 0 corrupt:  41%|####1     | 1330/3226 [00:01<00:01, 1321.17it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 1463 images, 1 backgrounds, 0 corrupt:  45%|####5     | 1463/3226 [00:01<00:01, 1322.66it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 1600 images, 1 backgrounds, 0 corrupt:  50%|####9     | 1600/3226 [00:01<00:01, 1328.54it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 1739 images, 1 backgrounds, 0 corrupt:  54%|#####3    | 1739/3226 [00:01<00:01, 1344.23it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 1885 images, 2 backgrounds, 0 corrupt:  58%|#####8    | 1885/3226 [00:01<00:00, 1369.50it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 2047 images, 2 backgrounds, 0 corrupt:  63%|######3   | 2047/3226 [00:01<00:00, 1439.56it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 2202 images, 2 backgrounds, 0 corrupt:  68%|######8   | 2202/3226 [00:01<00:00, 1471.34it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 2350 images, 2 backgrounds, 0 corrupt:  73%|#######2  | 2350/3226 [00:01<00:00, 1425.79it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 2493 images, 3 backgrounds, 0 corrupt:  77%|#######7  | 2493/3226 [00:01<00:00, 1413.36it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 2636 images, 3 backgrounds, 0 corrupt:  82%|########1 | 2636/3226 [00:01<00:00, 1410.62it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 2778 images, 3 backgrounds, 0 corrupt:  86%|########6 | 2778/3226 [00:02<00:00, 1393.38it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 2918 images, 3 backgrounds, 0 corrupt:  90%|######### | 2918/3226 [00:02<00:00, 1367.67it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 3055 images, 3 backgrounds, 0 corrupt:  95%|#########4| 3055/3226 [00:02<00:00, 1363.68it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 3192 images, 4 backgrounds, 0 corrupt:  99%|#########8| 3192/3226 [00:02<00:00, 1349.02it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Oak\\Documents\\git_projects\\fd_widerface_yolov8\\datasets\\WIDER-FACE-1\\test\\labels... 3226 images, 4 backgrounds, 0 corrupt: 100%|##########| 3226/3226 [00:02<00:00, 1355.46it/s]\n",
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/202 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 1/202 [00:00<01:34,  2.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   1%|          | 2/202 [00:00<00:56,  3.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   1%|1         | 3/202 [00:00<00:43,  4.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   2%|1         | 4/202 [00:00<00:37,  5.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   3%|2         | 6/202 [00:01<00:25,  7.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   4%|3         | 8/202 [00:01<00:21,  9.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   5%|4         | 10/202 [00:01<00:18, 10.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   6%|5         | 12/202 [00:01<00:16, 11.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   7%|6         | 14/202 [00:01<00:16, 11.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   8%|7         | 16/202 [00:01<00:16, 11.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   9%|8         | 18/202 [00:02<00:16, 11.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  10%|9         | 20/202 [00:02<00:15, 11.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  11%|#         | 22/202 [00:02<00:15, 11.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  12%|#1        | 24/202 [00:02<00:14, 11.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  13%|#2        | 26/202 [00:02<00:14, 12.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  14%|#3        | 28/202 [00:02<00:13, 12.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  15%|#4        | 30/202 [00:03<00:14, 12.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  16%|#5        | 32/202 [00:03<00:14, 12.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  17%|#6        | 34/202 [00:03<00:14, 11.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  18%|#7        | 36/202 [00:03<00:15, 10.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  19%|#8        | 38/202 [00:03<00:15, 10.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  20%|#9        | 40/202 [00:03<00:14, 11.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  21%|##        | 42/202 [00:04<00:13, 11.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  22%|##1       | 44/202 [00:04<00:13, 11.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  23%|##2       | 46/202 [00:04<00:12, 12.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  24%|##3       | 48/202 [00:04<00:12, 12.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  25%|##4       | 50/202 [00:04<00:12, 12.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  26%|##5       | 52/202 [00:04<00:12, 12.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  27%|##6       | 54/202 [00:05<00:11, 12.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  28%|##7       | 56/202 [00:05<00:11, 12.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  29%|##8       | 58/202 [00:05<00:11, 12.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  30%|##9       | 60/202 [00:05<00:10, 13.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  31%|###       | 62/202 [00:05<00:11, 12.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  32%|###1      | 64/202 [00:05<00:11, 12.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  33%|###2      | 66/202 [00:06<00:11, 11.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  34%|###3      | 68/202 [00:06<00:11, 12.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  35%|###4      | 70/202 [00:06<00:10, 12.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  36%|###5      | 72/202 [00:06<00:10, 11.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  37%|###6      | 74/202 [00:06<00:10, 12.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  38%|###7      | 76/202 [00:06<00:10, 11.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  39%|###8      | 78/202 [00:07<00:10, 12.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  40%|###9      | 80/202 [00:07<00:09, 12.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  41%|####      | 82/202 [00:07<00:09, 12.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  42%|####1     | 84/202 [00:07<00:09, 12.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  43%|####2     | 86/202 [00:07<00:09, 12.45it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  44%|####3     | 88/202 [00:07<00:09, 12.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  45%|####4     | 90/202 [00:07<00:09, 12.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  46%|####5     | 92/202 [00:08<00:08, 12.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  47%|####6     | 94/202 [00:08<00:08, 12.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  48%|####7     | 96/202 [00:08<00:08, 12.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  49%|####8     | 98/202 [00:08<00:08, 12.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  50%|####9     | 100/202 [00:08<00:08, 12.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  50%|#####     | 102/202 [00:08<00:07, 12.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  51%|#####1    | 104/202 [00:09<00:07, 12.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  52%|#####2    | 106/202 [00:09<00:07, 12.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  53%|#####3    | 108/202 [00:09<00:07, 12.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  54%|#####4    | 110/202 [00:09<00:07, 12.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  55%|#####5    | 112/202 [00:09<00:06, 12.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  56%|#####6    | 114/202 [00:09<00:06, 13.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  57%|#####7    | 116/202 [00:10<00:06, 12.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  58%|#####8    | 118/202 [00:10<00:06, 12.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  59%|#####9    | 120/202 [00:10<00:06, 11.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  60%|######    | 122/202 [00:10<00:06, 12.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  61%|######1   | 124/202 [00:10<00:06, 12.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  62%|######2   | 126/202 [00:10<00:06, 12.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  63%|######3   | 128/202 [00:11<00:06, 10.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  64%|######4   | 130/202 [00:11<00:07,  9.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  65%|######5   | 132/202 [00:11<00:06, 10.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  66%|######6   | 134/202 [00:11<00:06,  9.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  67%|######7   | 136/202 [00:11<00:06,  9.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  68%|######8   | 138/202 [00:12<00:06,  9.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  69%|######8   | 139/202 [00:12<00:06,  9.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  70%|######9   | 141/202 [00:12<00:05, 10.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  71%|#######   | 143/202 [00:12<00:05, 10.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  72%|#######1  | 145/202 [00:12<00:05, 11.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  73%|#######2  | 147/202 [00:12<00:04, 11.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  74%|#######3  | 149/202 [00:13<00:04, 10.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  75%|#######4  | 151/202 [00:13<00:04, 10.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  76%|#######5  | 153/202 [00:13<00:05,  9.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  77%|#######6  | 155/202 [00:13<00:04, 10.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  78%|#######7  | 157/202 [00:13<00:04, 10.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  79%|#######8  | 159/202 [00:14<00:04, 10.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  80%|#######9  | 161/202 [00:14<00:04, 10.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  81%|########  | 163/202 [00:14<00:03,  9.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  82%|########1 | 165/202 [00:14<00:03, 10.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  83%|########2 | 167/202 [00:14<00:03, 11.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  84%|########3 | 169/202 [00:15<00:02, 11.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  85%|########4 | 171/202 [00:15<00:02, 11.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  86%|########5 | 173/202 [00:15<00:02, 12.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  87%|########6 | 175/202 [00:15<00:02, 12.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  88%|########7 | 177/202 [00:15<00:01, 12.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  89%|########8 | 179/202 [00:15<00:01, 12.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  90%|########9 | 181/202 [00:15<00:01, 12.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  91%|######### | 183/202 [00:16<00:01, 12.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  92%|#########1| 185/202 [00:16<00:01, 11.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  93%|#########2| 187/202 [00:16<00:01, 12.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  94%|#########3| 189/202 [00:16<00:01, 12.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  95%|#########4| 191/202 [00:16<00:00, 11.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  96%|#########5| 193/202 [00:17<00:00, 10.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  97%|#########6| 195/202 [00:17<00:00,  9.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  98%|#########7| 197/202 [00:17<00:00,  9.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  99%|#########8| 199/202 [00:17<00:00, 10.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|#########9| 201/202 [00:17<00:00, 11.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|##########| 202/202 [00:17<00:00, 11.27it/s]\n"
     ]
    }
   ],
   "source": [
    "!yolo val model=\"model/best.pt\" data=\"yaml/data-test.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc9471d-97f4-48bd-9619-5a87e0152f02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'38_Tennis_Tennis_38_580_jpg.rf.997d19d1ee3a6b384fd95be22255f637.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loc = \"datasets/WIDER-FACE-1/valid/images/\"\n",
    "val = os.listdir(val_loc)\n",
    "val[1005]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a66a5db7-2646-4828-b0f7-8a63b617ed24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 🚀 Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/1 /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/valid/images/41_Swimming_Swimming_41_822_jpg.rf.f984914614bec7204f04f5fd110e1003.jpg: 640x640 5 faces, 7.7ms\n",
      "Speed: 5.5ms preprocess, 7.7ms inference, 1903.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001B[1mruns/detect/predict3\u001B[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# Running on a sample image\n",
    "!yolo predict model=\"model/best.pt\" source=\"datasets/WIDER-FACE-1/valid/images/41_Swimming_Swimming_41_822_jpg.rf.f984914614bec7204f04f5fd110e1003.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee88afcd-e734-4d8d-a780-cfd2699c33d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running Prediction on Video Footage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3518073f-2d8f-4ba3-82ec-7d36f301394e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=ek05M8eCk7M\n",
      "[youtube] ek05M8eCk7M: Downloading webpage\n",
      "[youtube] ek05M8eCk7M: Downloading ios player API JSON\n",
      "[youtube] ek05M8eCk7M: Downloading android player API JSON\n",
      "[youtube] ek05M8eCk7M: Downloading m3u8 information\n",
      "[info] ek05M8eCk7M: Downloading 1 format(s): 22\n",
      "[download] Destination: datasets/youtube_vid/[페이스캠4K] 뉴진스 하니 'Ditto' (NewJeans HANNI FaceCam) @SBS Inkigayo 230115.mp4\n",
      "[download] 100% of   39.58MiB in 00:00:02 at 16.51MiB/s    \n"
     ]
    }
   ],
   "source": [
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "# URLS = [\"https://www.youtube.com/watch?v=5F6j4e1C4Zk\"]  # Card\n",
    "# URLS = [\"https://www.youtube.com/watch?v=G_So72lFNIU\"]  # Card\n",
    "URLS = [\"https://www.youtube.com/watch?v=ek05M8eCk7M\"]  # Face\n",
    "\n",
    "\n",
    "dl_ops = {\n",
    "  'outtmpl': 'datasets/youtube_vid/%(title)s.%(ext)s'\n",
    "}\n",
    "\n",
    "with YoutubeDL(dl_ops) as ydl:\n",
    "    ydl.download(URLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739a2e1-2743-491b-a4da-fa064375007c",
   "metadata": {},
   "source": [
    "### Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d7bfc84-828b-4462-b74e-adc535210bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "def video_to_frames(input_loc, output_loc, num_frames=10):\n",
    "    \"\"\"Function to extract frames from input video file\n",
    "    and save them as separate frames in an output directory.\n",
    "    Args:\n",
    "        input_loc: Input video file.\n",
    "        output_loc: Output directory to save the frames.\n",
    "        num_frames: Desired number of equally spaced frames.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(output_loc)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # Log the time\n",
    "    time_start = time.time()\n",
    "\n",
    "    # Start capturing the feed\n",
    "    cap = cv2.VideoCapture(input_loc)\n",
    "\n",
    "    # Find the number of frames\n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "    print(\"Number of available frames: \", video_length)\n",
    "    print(\"Number of printing frames: \", min(num_frames, video_length))\n",
    "\n",
    "    # Calculate the step size to get equally spaced frames\n",
    "    step_size = max(1, video_length // num_frames)\n",
    "\n",
    "    count = 0\n",
    "    print(\"Converting video..\\n\")\n",
    "\n",
    "    # Start converting the video\n",
    "    while cap.isOpened():\n",
    "        # Extract the frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Check if the current frame should be captured\n",
    "        if count % step_size == 0:\n",
    "            # Write the results back to the output location.\n",
    "            cv2.imwrite(output_loc + \"/%#05d.jpg\" % (count//step_size + 1), frame)\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "        # If there are no more frames left\n",
    "        if count > video_length - 1:\n",
    "            # Log the time again\n",
    "            time_end = time.time()\n",
    "            # Release the feed\n",
    "            cap.release()\n",
    "            # Print stats\n",
    "            print(\"Done extracting frames.\\n%d frames extracted\" % (count//step_size))\n",
    "            print(\"It took %d seconds for conversion.\" % (time_end - time_start))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d30c5ec3-def7-4ae1-8339-3202d8e0fc24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[페이스캠4K] 뉴진스 하니 'Ditto' (NewJeans HANNI FaceCam) @SBS Inkigayo 230115.mp4\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_loc = \"datasets/youtube_vid/\"\n",
    "video = os.listdir(video_loc)\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "937f4ce9-fde2-4fe3-8a3f-f4feb679e27e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available frames:  8318\n",
      "Number of printing frames:  20\n",
      "Converting video..\n",
      "\n",
      "Done extracting frames.\n",
      "20 frames extracted\n",
      "It took 24 seconds for conversion.\n"
     ]
    }
   ],
   "source": [
    "input_loc = video_loc + video[0]\n",
    "output_loc = \"datasets/youtube_frames\"\n",
    "\n",
    "video_to_frames(input_loc, output_loc,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b29e6ee-174a-44b1-a875-e9fb4f02fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 🚀 Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00001.jpg: 384x640 3 faces, 234.9ms\n",
      "image 2/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00002.jpg: 384x640 3 faces, 7.0ms\n",
      "image 3/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00003.jpg: 384x640 4 faces, 8.2ms\n",
      "image 4/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00004.jpg: 384x640 1 face, 6.3ms\n",
      "image 5/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00005.jpg: 384x640 1 face, 6.1ms\n",
      "image 6/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00006.jpg: 384x640 2 faces, 6.1ms\n",
      "image 7/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00007.jpg: 384x640 1 face, 6.1ms\n",
      "image 8/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00008.jpg: 384x640 2 faces, 7.9ms\n",
      "image 9/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00009.jpg: 384x640 2 faces, 6.1ms\n",
      "image 10/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00010.jpg: 384x640 1 face, 6.3ms\n",
      "image 11/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00011.jpg: 384x640 2 faces, 6.0ms\n",
      "image 12/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00012.jpg: 384x640 1 face, 6.0ms\n",
      "image 13/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00013.jpg: 384x640 1 face, 6.1ms\n",
      "image 14/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00014.jpg: 384x640 1 face, 6.0ms\n",
      "image 15/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00015.jpg: 384x640 2 faces, 6.2ms\n",
      "image 16/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00016.jpg: 384x640 2 faces, 6.1ms\n",
      "image 17/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00017.jpg: 384x640 (no detections), 7.9ms\n",
      "image 18/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00018.jpg: 384x640 4 faces, 6.1ms\n",
      "image 19/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00019.jpg: 384x640 2 faces, 6.1ms\n",
      "image 20/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00020.jpg: 384x640 (no detections), 7.6ms\n",
      "image 21/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00021.jpg: 384x640 (no detections), 6.6ms\n",
      "Speed: 3.6ms preprocess, 17.4ms inference, 32.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001B[1mruns/detect/predict4\u001B[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "!yolo predict model=\"model/best.pt\" source='datasets/youtube_frames'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e79f94-de5e-4e76-a106-cafe654b58aa",
   "metadata": {},
   "source": [
    "## Creating Crop Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa686336-77b6-49ce-86fd-f251a5f19e04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00001.jpg: 384x640 3 faces, 8.9ms\n",
      "image 2/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00002.jpg: 384x640 3 faces, 5.7ms\n",
      "image 3/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00003.jpg: 384x640 4 faces, 5.6ms\n",
      "image 4/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00004.jpg: 384x640 1 face, 6.0ms\n",
      "image 5/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00005.jpg: 384x640 1 face, 5.9ms\n",
      "image 6/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00006.jpg: 384x640 2 faces, 5.6ms\n",
      "image 7/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00007.jpg: 384x640 1 face, 5.6ms\n",
      "image 8/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00008.jpg: 384x640 2 faces, 5.7ms\n",
      "image 9/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00009.jpg: 384x640 2 faces, 7.1ms\n",
      "image 10/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00010.jpg: 384x640 1 face, 5.9ms\n",
      "image 11/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00011.jpg: 384x640 2 faces, 5.8ms\n",
      "image 12/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00012.jpg: 384x640 1 face, 5.8ms\n",
      "image 13/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00013.jpg: 384x640 1 face, 5.6ms\n",
      "image 14/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00014.jpg: 384x640 1 face, 5.6ms\n",
      "image 15/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00015.jpg: 384x640 2 faces, 5.7ms\n",
      "image 16/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00016.jpg: 384x640 2 faces, 5.7ms\n",
      "image 17/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00017.jpg: 384x640 (no detections), 5.7ms\n",
      "image 18/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00018.jpg: 384x640 4 faces, 5.5ms\n",
      "image 19/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00019.jpg: 384x640 2 faces, 5.5ms\n",
      "image 20/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00020.jpg: 384x640 (no detections), 5.7ms\n",
      "image 21/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00021.jpg: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('model/best.pt')\n",
    "\n",
    "sample_img = \"/home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/\"\n",
    "results = model(sample_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf4523c4-265e-4b2b-96ab-fb58f53f6b74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00001.jpg'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b46a0954-0f71-4618-b964-86965a8f49fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "save_directory = \"imgs/cropface/\"\n",
    "# img = cv2.imread(sample_img)\n",
    "classNames = [\"face\"]\n",
    "\n",
    "def cropimgs(results, save_directory, expand_ratio = 0.2, CI = 0.7):\n",
    "    # results: list of results for all the images after running the model\n",
    "    # save_directory: location of output directory\n",
    "    # expand_ratio: expand the size of x1,x2 by expand_ratio percent. \n",
    "    # CI: confidence interval of prediction. Only crop the image if the confidence is higher than this threshold\n",
    "    \n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "        imgpath = r.path\n",
    "        img = cv2.imread(imgpath)\n",
    "        \n",
    "        cropstr = imgpath.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "        img_index = 0\n",
    "\n",
    "        for box in boxes:\n",
    "            # bounding box\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # convert to int values\n",
    "            \n",
    "            # Calculate expanded coordinates\n",
    "            x1 = max(0, int(x1 - expand_ratio * (x2 - x1)))\n",
    "            y1 = max(0, int(y1 - expand_ratio * (y2 - y1)))\n",
    "            x2 = min(img.shape[1], int(x2 + expand_ratio * (x2 - x1)))\n",
    "            y2 = min(img.shape[0], int(y2 + expand_ratio * (y2 - y1)))\n",
    "\n",
    "            # put box in cam\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "\n",
    "            # confidence\n",
    "            confidence = math.ceil((box.conf[0]*100))/100\n",
    "            # print(\"Confidence --->\",confidence)\n",
    "\n",
    "            # # class name\n",
    "            # cls = int(box.cls[0])\n",
    "            # print(\"Class name -->\", classNames[cls])\n",
    "\n",
    "            if classNames[cls] == \"face\" and confidence > CI:\n",
    "                # Crop the person from the image\n",
    "                cropped_person = img[y1:y2, x1:x2]\n",
    "\n",
    "                # Save the cropped person image\n",
    "                filename = f\"{save_directory}{cropstr}_{img_index}.jpg\"\n",
    "                cv2.imwrite(filename, cropped_person)\n",
    "                img_index += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65efdd40-2085-4b25-8ef5-3765fa52198f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cropimgs(results, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90ff0ada-bfb5-4b81-bf60-c7c79d7e5075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !yolo predict model=yolov8n.pt source='datasets/youtube_frames'"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-15:m116"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
