{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323a4995-14a3-4666-a69e-b8446472f606",
   "metadata": {},
   "source": [
    "## Downloading Widerface Dataset from Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f35e9c-5205-487a-9e7a-00603e01c14c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/fd_widerface_yolov8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "645098df-dcad-403b-b1df-bd45d6ea724d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in /opt/conda/lib/python3.10/site-packages (1.1.19)\n",
      "Requirement already satisfied: certifi==2023.7.22 in /opt/conda/lib/python3.10/site-packages (from roboflow) (2023.7.22)\n",
      "Requirement already satisfied: chardet==4.0.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.0.0)\n",
      "Requirement already satisfied: cycler==0.10.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.10.0)\n",
      "Requirement already satisfied: idna==2.10 in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.10)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.4.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from roboflow) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.24.4)\n",
      "Requirement already satisfied: opencv-python-headless==4.8.0.74 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.8.0.74)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from roboflow) (10.2.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.8.2)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.31.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: supervision in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.18.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.26.18)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.66.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from roboflow) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.10.1)\n",
      "Requirement already satisfied: python-magic in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.4.27)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (4.47.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->roboflow) (3.3.2)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from supervision->roboflow) (0.7.1)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from supervision->roboflow) (1.11.4)\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.11, to fix: `pip install ultralytics==8.0.196`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in WIDER-FACE-1 to yolov8:: 100%|██████████| 968279/968279 [00:19<00:00, 49608.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to WIDER-FACE-1 in yolov8:: 100%|██████████| 32215/32215 [00:04<00:00, 6697.66it/s] \n"
     ]
    }
   ],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"###\")\n",
    "project = rf.workspace(\"large-benchmark-datasets\").project(\"wider-face-ndtcz\")\n",
    "dataset = project.version(1).download(\"yolov8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83331c64-d4ab-4a8d-bf53-53af0fe2a0e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c1e26cb-10b5-4145-a240-9e559702af09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred 355/355 items from pretrained weights\n",
      "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "(225, 3157200, 3157184, 8.8575488)\n"
     ]
    }
   ],
   "source": [
    "# We will use yolov8n because it is smallest model size, and should be sufficient for non-complex task such as this.\n",
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8n.yaml').load('yolov8n.pt')\n",
    "\n",
    "print(model.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef3158a7-45cf-4bc2-92a1-77ddeb83c141",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.model.0.conv.weight\n",
      "model.model.0.bn.weight\n",
      "model.model.0.bn.bias\n",
      "model.model.1.conv.weight\n",
      "model.model.1.bn.weight\n",
      "model.model.1.bn.bias\n",
      "model.model.2.cv1.conv.weight\n",
      "model.model.2.cv1.bn.weight\n",
      "model.model.2.cv1.bn.bias\n",
      "model.model.2.cv2.conv.weight\n",
      "model.model.2.cv2.bn.weight\n",
      "model.model.2.cv2.bn.bias\n",
      "model.model.2.m.0.cv1.conv.weight\n",
      "model.model.2.m.0.cv1.bn.weight\n",
      "model.model.2.m.0.cv1.bn.bias\n",
      "model.model.2.m.0.cv2.conv.weight\n",
      "model.model.2.m.0.cv2.bn.weight\n",
      "model.model.2.m.0.cv2.bn.bias\n",
      "model.model.3.conv.weight\n",
      "model.model.3.bn.weight\n",
      "model.model.3.bn.bias\n",
      "model.model.4.cv1.conv.weight\n",
      "model.model.4.cv1.bn.weight\n",
      "model.model.4.cv1.bn.bias\n",
      "model.model.4.cv2.conv.weight\n",
      "model.model.4.cv2.bn.weight\n",
      "model.model.4.cv2.bn.bias\n",
      "model.model.4.m.0.cv1.conv.weight\n",
      "model.model.4.m.0.cv1.bn.weight\n",
      "model.model.4.m.0.cv1.bn.bias\n",
      "model.model.4.m.0.cv2.conv.weight\n",
      "model.model.4.m.0.cv2.bn.weight\n",
      "model.model.4.m.0.cv2.bn.bias\n",
      "model.model.4.m.1.cv1.conv.weight\n",
      "model.model.4.m.1.cv1.bn.weight\n",
      "model.model.4.m.1.cv1.bn.bias\n",
      "model.model.4.m.1.cv2.conv.weight\n",
      "model.model.4.m.1.cv2.bn.weight\n",
      "model.model.4.m.1.cv2.bn.bias\n",
      "model.model.5.conv.weight\n",
      "model.model.5.bn.weight\n",
      "model.model.5.bn.bias\n",
      "model.model.6.cv1.conv.weight\n",
      "model.model.6.cv1.bn.weight\n",
      "model.model.6.cv1.bn.bias\n",
      "model.model.6.cv2.conv.weight\n",
      "model.model.6.cv2.bn.weight\n",
      "model.model.6.cv2.bn.bias\n",
      "model.model.6.m.0.cv1.conv.weight\n",
      "model.model.6.m.0.cv1.bn.weight\n",
      "model.model.6.m.0.cv1.bn.bias\n",
      "model.model.6.m.0.cv2.conv.weight\n",
      "model.model.6.m.0.cv2.bn.weight\n",
      "model.model.6.m.0.cv2.bn.bias\n",
      "model.model.6.m.1.cv1.conv.weight\n",
      "model.model.6.m.1.cv1.bn.weight\n",
      "model.model.6.m.1.cv1.bn.bias\n",
      "model.model.6.m.1.cv2.conv.weight\n",
      "model.model.6.m.1.cv2.bn.weight\n",
      "model.model.6.m.1.cv2.bn.bias\n",
      "model.model.7.conv.weight\n",
      "model.model.7.bn.weight\n",
      "model.model.7.bn.bias\n",
      "model.model.8.cv1.conv.weight\n",
      "model.model.8.cv1.bn.weight\n",
      "model.model.8.cv1.bn.bias\n",
      "model.model.8.cv2.conv.weight\n",
      "model.model.8.cv2.bn.weight\n",
      "model.model.8.cv2.bn.bias\n",
      "model.model.8.m.0.cv1.conv.weight\n",
      "model.model.8.m.0.cv1.bn.weight\n",
      "model.model.8.m.0.cv1.bn.bias\n",
      "model.model.8.m.0.cv2.conv.weight\n",
      "model.model.8.m.0.cv2.bn.weight\n",
      "model.model.8.m.0.cv2.bn.bias\n",
      "model.model.9.cv1.conv.weight\n",
      "model.model.9.cv1.bn.weight\n",
      "model.model.9.cv1.bn.bias\n",
      "model.model.9.cv2.conv.weight\n",
      "model.model.9.cv2.bn.weight\n",
      "model.model.9.cv2.bn.bias\n",
      "model.model.12.cv1.conv.weight\n",
      "model.model.12.cv1.bn.weight\n",
      "model.model.12.cv1.bn.bias\n",
      "model.model.12.cv2.conv.weight\n",
      "model.model.12.cv2.bn.weight\n",
      "model.model.12.cv2.bn.bias\n",
      "model.model.12.m.0.cv1.conv.weight\n",
      "model.model.12.m.0.cv1.bn.weight\n",
      "model.model.12.m.0.cv1.bn.bias\n",
      "model.model.12.m.0.cv2.conv.weight\n",
      "model.model.12.m.0.cv2.bn.weight\n",
      "model.model.12.m.0.cv2.bn.bias\n",
      "model.model.15.cv1.conv.weight\n",
      "model.model.15.cv1.bn.weight\n",
      "model.model.15.cv1.bn.bias\n",
      "model.model.15.cv2.conv.weight\n",
      "model.model.15.cv2.bn.weight\n",
      "model.model.15.cv2.bn.bias\n",
      "model.model.15.m.0.cv1.conv.weight\n",
      "model.model.15.m.0.cv1.bn.weight\n",
      "model.model.15.m.0.cv1.bn.bias\n",
      "model.model.15.m.0.cv2.conv.weight\n",
      "model.model.15.m.0.cv2.bn.weight\n",
      "model.model.15.m.0.cv2.bn.bias\n",
      "model.model.16.conv.weight\n",
      "model.model.16.bn.weight\n",
      "model.model.16.bn.bias\n",
      "model.model.18.cv1.conv.weight\n",
      "model.model.18.cv1.bn.weight\n",
      "model.model.18.cv1.bn.bias\n",
      "model.model.18.cv2.conv.weight\n",
      "model.model.18.cv2.bn.weight\n",
      "model.model.18.cv2.bn.bias\n",
      "model.model.18.m.0.cv1.conv.weight\n",
      "model.model.18.m.0.cv1.bn.weight\n",
      "model.model.18.m.0.cv1.bn.bias\n",
      "model.model.18.m.0.cv2.conv.weight\n",
      "model.model.18.m.0.cv2.bn.weight\n",
      "model.model.18.m.0.cv2.bn.bias\n",
      "model.model.19.conv.weight\n",
      "model.model.19.bn.weight\n",
      "model.model.19.bn.bias\n",
      "model.model.21.cv1.conv.weight\n",
      "model.model.21.cv1.bn.weight\n",
      "model.model.21.cv1.bn.bias\n",
      "model.model.21.cv2.conv.weight\n",
      "model.model.21.cv2.bn.weight\n",
      "model.model.21.cv2.bn.bias\n",
      "model.model.21.m.0.cv1.conv.weight\n",
      "model.model.21.m.0.cv1.bn.weight\n",
      "model.model.21.m.0.cv1.bn.bias\n",
      "model.model.21.m.0.cv2.conv.weight\n",
      "model.model.21.m.0.cv2.bn.weight\n",
      "model.model.21.m.0.cv2.bn.bias\n",
      "model.model.22.cv2.0.0.conv.weight\n",
      "model.model.22.cv2.0.0.bn.weight\n",
      "model.model.22.cv2.0.0.bn.bias\n",
      "model.model.22.cv2.0.1.conv.weight\n",
      "model.model.22.cv2.0.1.bn.weight\n",
      "model.model.22.cv2.0.1.bn.bias\n",
      "model.model.22.cv2.0.2.weight\n",
      "model.model.22.cv2.0.2.bias\n",
      "model.model.22.cv2.1.0.conv.weight\n",
      "model.model.22.cv2.1.0.bn.weight\n",
      "model.model.22.cv2.1.0.bn.bias\n",
      "model.model.22.cv2.1.1.conv.weight\n",
      "model.model.22.cv2.1.1.bn.weight\n",
      "model.model.22.cv2.1.1.bn.bias\n",
      "model.model.22.cv2.1.2.weight\n",
      "model.model.22.cv2.1.2.bias\n",
      "model.model.22.cv2.2.0.conv.weight\n",
      "model.model.22.cv2.2.0.bn.weight\n",
      "model.model.22.cv2.2.0.bn.bias\n",
      "model.model.22.cv2.2.1.conv.weight\n",
      "model.model.22.cv2.2.1.bn.weight\n",
      "model.model.22.cv2.2.1.bn.bias\n",
      "model.model.22.cv2.2.2.weight\n",
      "model.model.22.cv2.2.2.bias\n",
      "model.model.22.cv3.0.0.conv.weight\n",
      "model.model.22.cv3.0.0.bn.weight\n",
      "model.model.22.cv3.0.0.bn.bias\n",
      "model.model.22.cv3.0.1.conv.weight\n",
      "model.model.22.cv3.0.1.bn.weight\n",
      "model.model.22.cv3.0.1.bn.bias\n",
      "model.model.22.cv3.0.2.weight\n",
      "model.model.22.cv3.0.2.bias\n",
      "model.model.22.cv3.1.0.conv.weight\n",
      "model.model.22.cv3.1.0.bn.weight\n",
      "model.model.22.cv3.1.0.bn.bias\n",
      "model.model.22.cv3.1.1.conv.weight\n",
      "model.model.22.cv3.1.1.bn.weight\n",
      "model.model.22.cv3.1.1.bn.bias\n",
      "model.model.22.cv3.1.2.weight\n",
      "model.model.22.cv3.1.2.bias\n",
      "model.model.22.cv3.2.0.conv.weight\n",
      "model.model.22.cv3.2.0.bn.weight\n",
      "model.model.22.cv3.2.0.bn.bias\n",
      "model.model.22.cv3.2.1.conv.weight\n",
      "model.model.22.cv3.2.1.bn.weight\n",
      "model.model.22.cv3.2.1.bn.bias\n",
      "model.model.22.cv3.2.2.weight\n",
      "model.model.22.cv3.2.2.bias\n",
      "model.model.22.dfl.conv.weight\n"
     ]
    }
   ],
   "source": [
    "# Check all the layers to identify if we should be freezing any of the layers during transfer learning\n",
    "for k, v in model.named_parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "118327ab-a796-4861-926f-80dadd7920ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo train model=yolov8n.pt data='data.yaml' epochs=20 imgsz=640 batch=16 name=train_face verbose=True plots=True save_period=5 workers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc07d23b-5be7-4640-82a0-47d1665b9369",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5674"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear memory\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3c60e-7658-4286-8f12-ab495cc382d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 🚀 Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data.yaml, epochs=20, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=5, cache=False, device=None, workers=8, project=None, name=train_face23, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train_face23\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "WARNING ⚠️ Comet installed but not initialized correctly, not logging this run. Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY \n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train_face23', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/train/labels.cache... 12877 images, 4 backgrounds, 0 corrupt: 100%|██████████| 12877/12877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/train/images/2_Demonstration_Protesters_2_231_jpg.rf.14b6ba970f3a34843cf41706cf802e37.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/train/images/37_Soccer_Soccer_37_851_jpg.rf.9626888f7c524f523f123df5c7417573.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/train/images/7_Cheering_Cheering_7_17_jpg.rf.f33362e0b02002a17ce25e9fcc6c6650.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/valid/labels.cache... 3226 images, 4 backgrounds, 0 corrupt: 100%|██████████| 3226/3226 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/valid/images/21_Festival_Festival_21_604_jpg.rf.f65a6018a5c9c5f0a3f93bbe84b46203.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train_face23/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train_face23\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      13.8G      1.646     0.9648      1.088         86        640: 100%|██████████| 805/805 [04:06<00:00,  3.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:33<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.782      0.492      0.562       0.29\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      11.9G      1.651     0.9748      1.096        216        640: 100%|██████████| 805/805 [04:26<00:00,  3.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.772      0.479      0.554      0.284\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      12.8G      1.657     0.9923      1.096        103        640: 100%|██████████| 805/805 [04:37<00:00,  2.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:36<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696       0.78      0.478      0.545      0.277\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      14.3G      1.659     0.9902      1.101        205        640: 100%|██████████| 805/805 [04:12<00:00,  3.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.786      0.483      0.554      0.291\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      12.1G      1.651      0.962      1.093         96        640: 100%|██████████| 805/805 [04:22<00:00,  3.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.791      0.505      0.575      0.297\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      12.1G      1.619     0.9387      1.085        119        640: 100%|██████████| 805/805 [03:59<00:00,  3.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:33<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696        0.8      0.509      0.584      0.301\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20      12.3G      1.608     0.9219       1.08        305        640: 100%|██████████| 805/805 [03:59<00:00,  3.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.813      0.512       0.59      0.308\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20      11.2G       1.61     0.9148      1.073        503        640: 100%|██████████| 805/805 [04:01<00:00,  3.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:32<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.808      0.523      0.597      0.314\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20      8.78G      1.579     0.8925      1.073        172        640: 100%|██████████| 805/805 [04:01<00:00,  3.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:36<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.808      0.525      0.602      0.315\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20      9.21G       1.57     0.8754      1.067        116        640: 100%|██████████| 805/805 [04:07<00:00,  3.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.812      0.528      0.608      0.317\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20      10.9G      1.548     0.8477      1.073         87        640: 100%|██████████| 805/805 [04:10<00:00,  3.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:36<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.808      0.523      0.597      0.313\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20      11.7G      1.526     0.8271      1.068         46        640: 100%|██████████| 805/805 [04:34<00:00,  2.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:35<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.818       0.53      0.608      0.318\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      12.1G      1.516       0.81      1.061        100        640: 100%|██████████| 805/805 [04:28<00:00,  3.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:35<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.813      0.535      0.611      0.325\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20      10.3G      1.504     0.7961      1.054        373        640: 100%|██████████| 805/805 [04:22<00:00,  3.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:35<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.825      0.539      0.619       0.33\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20      12.1G      1.494     0.7855      1.054        163        640: 100%|██████████| 805/805 [04:32<00:00,  2.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.825      0.544      0.626      0.334\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      13.5G      1.481     0.7738      1.047        109        640: 100%|██████████| 805/805 [04:20<00:00,  3.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.824       0.55      0.628      0.334\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      13.9G       1.47     0.7619      1.046        118        640: 100%|██████████| 805/805 [04:36<00:00,  2.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:34<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.826      0.552      0.631       0.34\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      12.3G      1.456     0.7489       1.04         78        640: 100%|██████████| 805/805 [04:30<00:00,  2.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:35<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696       0.83      0.552      0.634      0.342\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20      13.1G      1.449     0.7364      1.038         64        640: 100%|██████████| 805/805 [04:43<00:00,  2.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:35<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696       0.83      0.554      0.636      0.343\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20        13G      1.432     0.7233      1.032        133        640: 100%|██████████| 805/805 [04:17<00:00,  3.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 101/101 [00:35<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696       0.83      0.558      0.638      0.343\n",
      "\n",
      "20 epochs completed in 1.654 hours.\n",
      "Optimizer stripped from runs/detect/train_face23/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train_face23/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train_face23/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.11 🚀 Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   3%|▎         | 3/101 [00:02<01:47,  1.09s/it]"
     ]
    }
   ],
   "source": [
    "results = model.train(data='data.yaml', epochs=20, imgsz=640, batch=16, name=\"train_face2\", verbose=True, plots=True, save_period=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0664e979-004b-4c99-8147-90dbfc289f99",
   "metadata": {},
   "source": [
    "Note that some of the layers within the model has been fused to optimize model performance:\n",
    "\n",
    "https://docs.ultralytics.com/reference/engine/model/#ultralytics.engine.model.Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6a382-4e28-4353-9170-66960e742bbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f0c789-3872-48d2-8761-f7faa43324bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !yolo val model=\"runs/detect/train_card_v3/weights/best.pt\" data=data2-test.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc9471d-97f4-48bd-9619-5a87e0152f02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'38_Tennis_Tennis_38_580_jpg.rf.997d19d1ee3a6b384fd95be22255f637.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loc = \"datasets/WIDER-FACE-1/valid/images/\"\n",
    "val = os.listdir(val_loc)\n",
    "val[1005]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a66a5db7-2646-4828-b0f7-8a63b617ed24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 🚀 Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/1 /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/valid/images/41_Swimming_Swimming_41_822_jpg.rf.f984914614bec7204f04f5fd110e1003.jpg: 640x640 5 faces, 7.7ms\n",
      "Speed: 5.5ms preprocess, 7.7ms inference, 1903.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict3\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# Running on a sample image\n",
    "!yolo predict model=\"model/best.pt\" source=\"datasets/WIDER-FACE-1/valid/images/41_Swimming_Swimming_41_822_jpg.rf.f984914614bec7204f04f5fd110e1003.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee88afcd-e734-4d8d-a780-cfd2699c33d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running Prediction on Video Footage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3518073f-2d8f-4ba3-82ec-7d36f301394e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=ek05M8eCk7M\n",
      "[youtube] ek05M8eCk7M: Downloading webpage\n",
      "[youtube] ek05M8eCk7M: Downloading ios player API JSON\n",
      "[youtube] ek05M8eCk7M: Downloading android player API JSON\n",
      "[youtube] ek05M8eCk7M: Downloading m3u8 information\n",
      "[info] ek05M8eCk7M: Downloading 1 format(s): 22\n",
      "[download] Destination: datasets/youtube_vid/[페이스캠4K] 뉴진스 하니 'Ditto' (NewJeans HANNI FaceCam) @SBS Inkigayo 230115.mp4\n",
      "[download] 100% of   39.58MiB in 00:00:02 at 16.51MiB/s    \n"
     ]
    }
   ],
   "source": [
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "# URLS = [\"https://www.youtube.com/watch?v=5F6j4e1C4Zk\"]  # Card\n",
    "# URLS = [\"https://www.youtube.com/watch?v=G_So72lFNIU\"]  # Card\n",
    "URLS = [\"https://www.youtube.com/watch?v=ek05M8eCk7M\"]  # Face\n",
    "\n",
    "\n",
    "dl_ops = {\n",
    "  'outtmpl': 'datasets/youtube_vid/%(title)s.%(ext)s'\n",
    "}\n",
    "\n",
    "with YoutubeDL(dl_ops) as ydl:\n",
    "    ydl.download(URLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739a2e1-2743-491b-a4da-fa064375007c",
   "metadata": {},
   "source": [
    "### Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d7bfc84-828b-4462-b74e-adc535210bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "def video_to_frames(input_loc, output_loc, num_frames=10):\n",
    "    \"\"\"Function to extract frames from input video file\n",
    "    and save them as separate frames in an output directory.\n",
    "    Args:\n",
    "        input_loc: Input video file.\n",
    "        output_loc: Output directory to save the frames.\n",
    "        num_frames: Desired number of equally spaced frames.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(output_loc)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # Log the time\n",
    "    time_start = time.time()\n",
    "\n",
    "    # Start capturing the feed\n",
    "    cap = cv2.VideoCapture(input_loc)\n",
    "\n",
    "    # Find the number of frames\n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "    print(\"Number of available frames: \", video_length)\n",
    "    print(\"Number of printing frames: \", min(num_frames, video_length))\n",
    "\n",
    "    # Calculate the step size to get equally spaced frames\n",
    "    step_size = max(1, video_length // num_frames)\n",
    "\n",
    "    count = 0\n",
    "    print(\"Converting video..\\n\")\n",
    "\n",
    "    # Start converting the video\n",
    "    while cap.isOpened():\n",
    "        # Extract the frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Check if the current frame should be captured\n",
    "        if count % step_size == 0:\n",
    "            # Write the results back to the output location.\n",
    "            cv2.imwrite(output_loc + \"/%#05d.jpg\" % (count//step_size + 1), frame)\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "        # If there are no more frames left\n",
    "        if count > video_length - 1:\n",
    "            # Log the time again\n",
    "            time_end = time.time()\n",
    "            # Release the feed\n",
    "            cap.release()\n",
    "            # Print stats\n",
    "            print(\"Done extracting frames.\\n%d frames extracted\" % (count//step_size))\n",
    "            print(\"It took %d seconds for conversion.\" % (time_end - time_start))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d30c5ec3-def7-4ae1-8339-3202d8e0fc24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[페이스캠4K] 뉴진스 하니 'Ditto' (NewJeans HANNI FaceCam) @SBS Inkigayo 230115.mp4\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_loc = \"datasets/youtube_vid/\"\n",
    "video = os.listdir(video_loc)\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "937f4ce9-fde2-4fe3-8a3f-f4feb679e27e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available frames:  8318\n",
      "Number of printing frames:  20\n",
      "Converting video..\n",
      "\n",
      "Done extracting frames.\n",
      "20 frames extracted\n",
      "It took 24 seconds for conversion.\n"
     ]
    }
   ],
   "source": [
    "input_loc = video_loc + video[0]\n",
    "output_loc = \"datasets/youtube_frames\"\n",
    "\n",
    "video_to_frames(input_loc, output_loc,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b29e6ee-174a-44b1-a875-e9fb4f02fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 🚀 Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00001.jpg: 384x640 3 faces, 234.9ms\n",
      "image 2/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00002.jpg: 384x640 3 faces, 7.0ms\n",
      "image 3/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00003.jpg: 384x640 4 faces, 8.2ms\n",
      "image 4/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00004.jpg: 384x640 1 face, 6.3ms\n",
      "image 5/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00005.jpg: 384x640 1 face, 6.1ms\n",
      "image 6/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00006.jpg: 384x640 2 faces, 6.1ms\n",
      "image 7/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00007.jpg: 384x640 1 face, 6.1ms\n",
      "image 8/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00008.jpg: 384x640 2 faces, 7.9ms\n",
      "image 9/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00009.jpg: 384x640 2 faces, 6.1ms\n",
      "image 10/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00010.jpg: 384x640 1 face, 6.3ms\n",
      "image 11/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00011.jpg: 384x640 2 faces, 6.0ms\n",
      "image 12/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00012.jpg: 384x640 1 face, 6.0ms\n",
      "image 13/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00013.jpg: 384x640 1 face, 6.1ms\n",
      "image 14/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00014.jpg: 384x640 1 face, 6.0ms\n",
      "image 15/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00015.jpg: 384x640 2 faces, 6.2ms\n",
      "image 16/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00016.jpg: 384x640 2 faces, 6.1ms\n",
      "image 17/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00017.jpg: 384x640 (no detections), 7.9ms\n",
      "image 18/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00018.jpg: 384x640 4 faces, 6.1ms\n",
      "image 19/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00019.jpg: 384x640 2 faces, 6.1ms\n",
      "image 20/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00020.jpg: 384x640 (no detections), 7.6ms\n",
      "image 21/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00021.jpg: 384x640 (no detections), 6.6ms\n",
      "Speed: 3.6ms preprocess, 17.4ms inference, 32.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict4\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "!yolo predict model=\"model/best.pt\" source='datasets/youtube_frames'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e79f94-de5e-4e76-a106-cafe654b58aa",
   "metadata": {},
   "source": [
    "## Creating Crop Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa686336-77b6-49ce-86fd-f251a5f19e04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00001.jpg: 384x640 3 faces, 8.9ms\n",
      "image 2/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00002.jpg: 384x640 3 faces, 5.7ms\n",
      "image 3/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00003.jpg: 384x640 4 faces, 5.6ms\n",
      "image 4/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00004.jpg: 384x640 1 face, 6.0ms\n",
      "image 5/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00005.jpg: 384x640 1 face, 5.9ms\n",
      "image 6/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00006.jpg: 384x640 2 faces, 5.6ms\n",
      "image 7/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00007.jpg: 384x640 1 face, 5.6ms\n",
      "image 8/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00008.jpg: 384x640 2 faces, 5.7ms\n",
      "image 9/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00009.jpg: 384x640 2 faces, 7.1ms\n",
      "image 10/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00010.jpg: 384x640 1 face, 5.9ms\n",
      "image 11/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00011.jpg: 384x640 2 faces, 5.8ms\n",
      "image 12/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00012.jpg: 384x640 1 face, 5.8ms\n",
      "image 13/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00013.jpg: 384x640 1 face, 5.6ms\n",
      "image 14/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00014.jpg: 384x640 1 face, 5.6ms\n",
      "image 15/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00015.jpg: 384x640 2 faces, 5.7ms\n",
      "image 16/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00016.jpg: 384x640 2 faces, 5.7ms\n",
      "image 17/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00017.jpg: 384x640 (no detections), 5.7ms\n",
      "image 18/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00018.jpg: 384x640 4 faces, 5.5ms\n",
      "image 19/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00019.jpg: 384x640 2 faces, 5.5ms\n",
      "image 20/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00020.jpg: 384x640 (no detections), 5.7ms\n",
      "image 21/21 /home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00021.jpg: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('model/best.pt')\n",
    "\n",
    "sample_img = \"/home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/\"\n",
    "results = model(sample_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf4523c4-265e-4b2b-96ab-fb58f53f6b74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/fd_widerface_yolov8/datasets/youtube_frames/00001.jpg'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b46a0954-0f71-4618-b964-86965a8f49fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "save_directory = \"imgs/cropface/\"\n",
    "# img = cv2.imread(sample_img)\n",
    "classNames = [\"face\"]\n",
    "\n",
    "def cropimgs(results, save_directory, expand_ratio = 0.2, CI = 0.7):\n",
    "    # results: list of results for all the images after running the model\n",
    "    # save_directory: location of output directory\n",
    "    # expand_ratio: expand the size of x1,x2 by expand_ratio percent. \n",
    "    # CI: confidence interval of prediction. Only crop the image if the confidence is higher than this threshold\n",
    "    \n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "        imgpath = r.path\n",
    "        img = cv2.imread(imgpath)\n",
    "        \n",
    "        cropstr = imgpath.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "        img_index = 0\n",
    "\n",
    "        for box in boxes:\n",
    "            # bounding box\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # convert to int values\n",
    "            \n",
    "            # Calculate expanded coordinates\n",
    "            x1 = max(0, int(x1 - expand_ratio * (x2 - x1)))\n",
    "            y1 = max(0, int(y1 - expand_ratio * (y2 - y1)))\n",
    "            x2 = min(img.shape[1], int(x2 + expand_ratio * (x2 - x1)))\n",
    "            y2 = min(img.shape[0], int(y2 + expand_ratio * (y2 - y1)))\n",
    "\n",
    "            # put box in cam\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "\n",
    "            # confidence\n",
    "            confidence = math.ceil((box.conf[0]*100))/100\n",
    "            # print(\"Confidence --->\",confidence)\n",
    "\n",
    "            # # class name\n",
    "            # cls = int(box.cls[0])\n",
    "            # print(\"Class name -->\", classNames[cls])\n",
    "\n",
    "            if classNames[cls] == \"face\" and confidence > CI:\n",
    "                # Crop the person from the image\n",
    "                cropped_person = img[y1:y2, x1:x2]\n",
    "\n",
    "                # Save the cropped person image\n",
    "                filename = f\"{save_directory}{cropstr}_{img_index}.jpg\"\n",
    "                cv2.imwrite(filename, cropped_person)\n",
    "                img_index += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65efdd40-2085-4b25-8ef5-3765fa52198f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cropimgs(results, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384251f-6585-4e1f-95b1-16413abf7001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2a202-7df9-40d3-8c51-5ba5917abe43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90ff0ada-bfb5-4b81-bf60-c7c79d7e5075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !yolo predict model=yolov8n.pt source='datasets/youtube_frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dadc19-0fa1-4c64-979f-325754f5a6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-15:m116"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
