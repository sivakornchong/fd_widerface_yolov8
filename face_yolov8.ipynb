{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323a4995-14a3-4666-a69e-b8446472f606",
   "metadata": {},
   "source": [
    "## Downloading Widerface Dataset from Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3f35e9c-5205-487a-9e7a-00603e01c14c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/fd_widerface_yolov8'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "645098df-dcad-403b-b1df-bd45d6ea724d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in /opt/conda/lib/python3.10/site-packages (1.1.19)\n",
      "Requirement already satisfied: certifi==2023.7.22 in /opt/conda/lib/python3.10/site-packages (from roboflow) (2023.7.22)\n",
      "Requirement already satisfied: chardet==4.0.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.0.0)\n",
      "Requirement already satisfied: cycler==0.10.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.10.0)\n",
      "Requirement already satisfied: idna==2.10 in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.10)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.4.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from roboflow) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.24.4)\n",
      "Requirement already satisfied: opencv-python-headless==4.8.0.74 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.8.0.74)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from roboflow) (10.2.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.8.2)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.31.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: supervision in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.18.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.26.18)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.66.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from roboflow) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.10.1)\n",
      "Requirement already satisfied: python-magic in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.4.27)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (4.47.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->roboflow) (3.3.2)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from supervision->roboflow) (0.7.1)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from supervision->roboflow) (1.11.4)\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.11, to fix: `pip install ultralytics==8.0.196`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in WIDER-FACE-1 to yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 968279/968279 [00:19<00:00, 49608.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to WIDER-FACE-1 in yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32215/32215 [00:04<00:00, 6697.66it/s] \n"
     ]
    }
   ],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"###\")\n",
    "project = rf.workspace(\"large-benchmark-datasets\").project(\"wider-face-ndtcz\")\n",
    "dataset = project.version(1).download(\"yolov8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83331c64-d4ab-4a8d-bf53-53af0fe2a0e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-summary\n",
      "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: torch-summary\n",
      "Successfully installed torch-summary-1.4.5\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c1e26cb-10b5-4145-a240-9e559702af09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.23M/6.23M [00:00<00:00, 109MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary: 225 layers, 3157200 parameters, 0 gradients, 8.9 GFLOPs\n",
      "(225, 3157200, 0, 8.8575488)\n"
     ]
    }
   ],
   "source": [
    "# We will use yolov8n because it is smallest model size, and should be sufficient for non-complex task such as this.\n",
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "print(model.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef3158a7-45cf-4bc2-92a1-77ddeb83c141",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.model.0.conv.weight\n",
      "model.model.0.bn.weight\n",
      "model.model.0.bn.bias\n",
      "model.model.1.conv.weight\n",
      "model.model.1.bn.weight\n",
      "model.model.1.bn.bias\n",
      "model.model.2.cv1.conv.weight\n",
      "model.model.2.cv1.bn.weight\n",
      "model.model.2.cv1.bn.bias\n",
      "model.model.2.cv2.conv.weight\n",
      "model.model.2.cv2.bn.weight\n",
      "model.model.2.cv2.bn.bias\n",
      "model.model.2.m.0.cv1.conv.weight\n",
      "model.model.2.m.0.cv1.bn.weight\n",
      "model.model.2.m.0.cv1.bn.bias\n",
      "model.model.2.m.0.cv2.conv.weight\n",
      "model.model.2.m.0.cv2.bn.weight\n",
      "model.model.2.m.0.cv2.bn.bias\n",
      "model.model.3.conv.weight\n",
      "model.model.3.bn.weight\n",
      "model.model.3.bn.bias\n",
      "model.model.4.cv1.conv.weight\n",
      "model.model.4.cv1.bn.weight\n",
      "model.model.4.cv1.bn.bias\n",
      "model.model.4.cv2.conv.weight\n",
      "model.model.4.cv2.bn.weight\n",
      "model.model.4.cv2.bn.bias\n",
      "model.model.4.m.0.cv1.conv.weight\n",
      "model.model.4.m.0.cv1.bn.weight\n",
      "model.model.4.m.0.cv1.bn.bias\n",
      "model.model.4.m.0.cv2.conv.weight\n",
      "model.model.4.m.0.cv2.bn.weight\n",
      "model.model.4.m.0.cv2.bn.bias\n",
      "model.model.4.m.1.cv1.conv.weight\n",
      "model.model.4.m.1.cv1.bn.weight\n",
      "model.model.4.m.1.cv1.bn.bias\n",
      "model.model.4.m.1.cv2.conv.weight\n",
      "model.model.4.m.1.cv2.bn.weight\n",
      "model.model.4.m.1.cv2.bn.bias\n",
      "model.model.5.conv.weight\n",
      "model.model.5.bn.weight\n",
      "model.model.5.bn.bias\n",
      "model.model.6.cv1.conv.weight\n",
      "model.model.6.cv1.bn.weight\n",
      "model.model.6.cv1.bn.bias\n",
      "model.model.6.cv2.conv.weight\n",
      "model.model.6.cv2.bn.weight\n",
      "model.model.6.cv2.bn.bias\n",
      "model.model.6.m.0.cv1.conv.weight\n",
      "model.model.6.m.0.cv1.bn.weight\n",
      "model.model.6.m.0.cv1.bn.bias\n",
      "model.model.6.m.0.cv2.conv.weight\n",
      "model.model.6.m.0.cv2.bn.weight\n",
      "model.model.6.m.0.cv2.bn.bias\n",
      "model.model.6.m.1.cv1.conv.weight\n",
      "model.model.6.m.1.cv1.bn.weight\n",
      "model.model.6.m.1.cv1.bn.bias\n",
      "model.model.6.m.1.cv2.conv.weight\n",
      "model.model.6.m.1.cv2.bn.weight\n",
      "model.model.6.m.1.cv2.bn.bias\n",
      "model.model.7.conv.weight\n",
      "model.model.7.bn.weight\n",
      "model.model.7.bn.bias\n",
      "model.model.8.cv1.conv.weight\n",
      "model.model.8.cv1.bn.weight\n",
      "model.model.8.cv1.bn.bias\n",
      "model.model.8.cv2.conv.weight\n",
      "model.model.8.cv2.bn.weight\n",
      "model.model.8.cv2.bn.bias\n",
      "model.model.8.m.0.cv1.conv.weight\n",
      "model.model.8.m.0.cv1.bn.weight\n",
      "model.model.8.m.0.cv1.bn.bias\n",
      "model.model.8.m.0.cv2.conv.weight\n",
      "model.model.8.m.0.cv2.bn.weight\n",
      "model.model.8.m.0.cv2.bn.bias\n",
      "model.model.9.cv1.conv.weight\n",
      "model.model.9.cv1.bn.weight\n",
      "model.model.9.cv1.bn.bias\n",
      "model.model.9.cv2.conv.weight\n",
      "model.model.9.cv2.bn.weight\n",
      "model.model.9.cv2.bn.bias\n",
      "model.model.12.cv1.conv.weight\n",
      "model.model.12.cv1.bn.weight\n",
      "model.model.12.cv1.bn.bias\n",
      "model.model.12.cv2.conv.weight\n",
      "model.model.12.cv2.bn.weight\n",
      "model.model.12.cv2.bn.bias\n",
      "model.model.12.m.0.cv1.conv.weight\n",
      "model.model.12.m.0.cv1.bn.weight\n",
      "model.model.12.m.0.cv1.bn.bias\n",
      "model.model.12.m.0.cv2.conv.weight\n",
      "model.model.12.m.0.cv2.bn.weight\n",
      "model.model.12.m.0.cv2.bn.bias\n",
      "model.model.15.cv1.conv.weight\n",
      "model.model.15.cv1.bn.weight\n",
      "model.model.15.cv1.bn.bias\n",
      "model.model.15.cv2.conv.weight\n",
      "model.model.15.cv2.bn.weight\n",
      "model.model.15.cv2.bn.bias\n",
      "model.model.15.m.0.cv1.conv.weight\n",
      "model.model.15.m.0.cv1.bn.weight\n",
      "model.model.15.m.0.cv1.bn.bias\n",
      "model.model.15.m.0.cv2.conv.weight\n",
      "model.model.15.m.0.cv2.bn.weight\n",
      "model.model.15.m.0.cv2.bn.bias\n",
      "model.model.16.conv.weight\n",
      "model.model.16.bn.weight\n",
      "model.model.16.bn.bias\n",
      "model.model.18.cv1.conv.weight\n",
      "model.model.18.cv1.bn.weight\n",
      "model.model.18.cv1.bn.bias\n",
      "model.model.18.cv2.conv.weight\n",
      "model.model.18.cv2.bn.weight\n",
      "model.model.18.cv2.bn.bias\n",
      "model.model.18.m.0.cv1.conv.weight\n",
      "model.model.18.m.0.cv1.bn.weight\n",
      "model.model.18.m.0.cv1.bn.bias\n",
      "model.model.18.m.0.cv2.conv.weight\n",
      "model.model.18.m.0.cv2.bn.weight\n",
      "model.model.18.m.0.cv2.bn.bias\n",
      "model.model.19.conv.weight\n",
      "model.model.19.bn.weight\n",
      "model.model.19.bn.bias\n",
      "model.model.21.cv1.conv.weight\n",
      "model.model.21.cv1.bn.weight\n",
      "model.model.21.cv1.bn.bias\n",
      "model.model.21.cv2.conv.weight\n",
      "model.model.21.cv2.bn.weight\n",
      "model.model.21.cv2.bn.bias\n",
      "model.model.21.m.0.cv1.conv.weight\n",
      "model.model.21.m.0.cv1.bn.weight\n",
      "model.model.21.m.0.cv1.bn.bias\n",
      "model.model.21.m.0.cv2.conv.weight\n",
      "model.model.21.m.0.cv2.bn.weight\n",
      "model.model.21.m.0.cv2.bn.bias\n",
      "model.model.22.cv2.0.0.conv.weight\n",
      "model.model.22.cv2.0.0.bn.weight\n",
      "model.model.22.cv2.0.0.bn.bias\n",
      "model.model.22.cv2.0.1.conv.weight\n",
      "model.model.22.cv2.0.1.bn.weight\n",
      "model.model.22.cv2.0.1.bn.bias\n",
      "model.model.22.cv2.0.2.weight\n",
      "model.model.22.cv2.0.2.bias\n",
      "model.model.22.cv2.1.0.conv.weight\n",
      "model.model.22.cv2.1.0.bn.weight\n",
      "model.model.22.cv2.1.0.bn.bias\n",
      "model.model.22.cv2.1.1.conv.weight\n",
      "model.model.22.cv2.1.1.bn.weight\n",
      "model.model.22.cv2.1.1.bn.bias\n",
      "model.model.22.cv2.1.2.weight\n",
      "model.model.22.cv2.1.2.bias\n",
      "model.model.22.cv2.2.0.conv.weight\n",
      "model.model.22.cv2.2.0.bn.weight\n",
      "model.model.22.cv2.2.0.bn.bias\n",
      "model.model.22.cv2.2.1.conv.weight\n",
      "model.model.22.cv2.2.1.bn.weight\n",
      "model.model.22.cv2.2.1.bn.bias\n",
      "model.model.22.cv2.2.2.weight\n",
      "model.model.22.cv2.2.2.bias\n",
      "model.model.22.cv3.0.0.conv.weight\n",
      "model.model.22.cv3.0.0.bn.weight\n",
      "model.model.22.cv3.0.0.bn.bias\n",
      "model.model.22.cv3.0.1.conv.weight\n",
      "model.model.22.cv3.0.1.bn.weight\n",
      "model.model.22.cv3.0.1.bn.bias\n",
      "model.model.22.cv3.0.2.weight\n",
      "model.model.22.cv3.0.2.bias\n",
      "model.model.22.cv3.1.0.conv.weight\n",
      "model.model.22.cv3.1.0.bn.weight\n",
      "model.model.22.cv3.1.0.bn.bias\n",
      "model.model.22.cv3.1.1.conv.weight\n",
      "model.model.22.cv3.1.1.bn.weight\n",
      "model.model.22.cv3.1.1.bn.bias\n",
      "model.model.22.cv3.1.2.weight\n",
      "model.model.22.cv3.1.2.bias\n",
      "model.model.22.cv3.2.0.conv.weight\n",
      "model.model.22.cv3.2.0.bn.weight\n",
      "model.model.22.cv3.2.0.bn.bias\n",
      "model.model.22.cv3.2.1.conv.weight\n",
      "model.model.22.cv3.2.1.bn.weight\n",
      "model.model.22.cv3.2.1.bn.bias\n",
      "model.model.22.cv3.2.2.weight\n",
      "model.model.22.cv3.2.2.bias\n",
      "model.model.22.dfl.conv.weight\n"
     ]
    }
   ],
   "source": [
    "# Check all the layers to identify if we should be freezing any of the layers during transfer learning\n",
    "for k, v in model.named_parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "118327ab-a796-4861-926f-80dadd7920ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo train model=yolov8n.pt data='data.yaml' epochs=20 imgsz=640 batch=16 name=train_face verbose=True plots=True save_period=5 workers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07d23b-5be7-4640-82a0-47d1665b9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3c60e-7658-4286-8f12-ab495cc382d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 üöÄ Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data.yaml, epochs=20, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=5, cache=False, device=None, workers=2, project=None, name=train_face22, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train_face22\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "WARNING ‚ö†Ô∏è Comet installed but not initialized correctly, not logging this run. Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY \n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train_face22', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/train/labels.cache... 12877 images, 4 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12877/12877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/train/images/2_Demonstration_Protesters_2_231_jpg.rf.14b6ba970f3a34843cf41706cf802e37.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/train/images/37_Soccer_Soccer_37_851_jpg.rf.9626888f7c524f523f123df5c7417573.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/train/images/7_Cheering_Cheering_7_17_jpg.rf.f33362e0b02002a17ce25e9fcc6c6650.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/valid/labels.cache... 3226 images, 4 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3226/3226 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/valid/images/21_Festival_Festival_21_604_jpg.rf.f65a6018a5c9c5f0a3f93bbe84b46203.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train_face22/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train_face22\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      12.2G      1.786      1.205      1.132         86        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 805/805 [04:41<00:00,  2.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:33<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.732      0.434      0.491      0.232\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      10.9G       1.77      1.134      1.139        325        640:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 646/805 [03:34<00:50,  3.14it/s]"
     ]
    }
   ],
   "source": [
    "results = model.train(data='data.yaml', epochs=20, imgsz=640, batch=16, name=\"train_face2\", verbose=True, plots=True, save_period=5, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2867516-6742-4bfe-a49e-37d846cc96c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c29dd-39df-472e-95ae-d795132de637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8c6a382-4e28-4353-9170-66960e742bbc",
   "metadata": {},
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "84f0c789-3872-48d2-8761-f7faa43324bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 üöÄ Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "Model summary (fused): 168 layers, 3015788 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter/datasets/Playing-Cards-4/test/labels... 1010 images,\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/jupyter/datasets/Playing-Cards-4/test/labels.cache\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       1010       4040      0.925      0.927      0.974      0.634\n",
      "                   10C       1010         77      0.972      0.987      0.991      0.669\n",
      "                   10D       1010         74      0.914          1      0.989      0.666\n",
      "                   10H       1010         54      0.995      0.963      0.994      0.698\n",
      "                   10S       1010         86      0.975          1      0.994      0.645\n",
      "                    2C       1010         86      0.907      0.902       0.96      0.656\n",
      "                    2D       1010         68      0.834      0.926      0.975      0.619\n",
      "                    2H       1010         85      0.929      0.894      0.976      0.644\n",
      "                    2S       1010         84       0.93      0.951      0.972      0.641\n",
      "                    3C       1010         73       0.86      0.922      0.968      0.688\n",
      "                    3D       1010         75      0.905      0.867      0.947       0.63\n",
      "                    3H       1010         88      0.904      0.909      0.969      0.638\n",
      "                    3S       1010         93      0.891      0.876       0.96      0.631\n",
      "                    4C       1010         67      0.969       0.91      0.984      0.636\n",
      "                    4D       1010        102       0.96       0.98      0.992      0.619\n",
      "                    4H       1010         86      0.953      0.942      0.985      0.613\n",
      "                    4S       1010         89      0.989      0.973      0.988      0.641\n",
      "                    5C       1010         94      0.934      0.901      0.984      0.623\n",
      "                    5D       1010         68      0.908      0.912      0.963      0.637\n",
      "                    5H       1010         65      0.874      0.958       0.97      0.647\n",
      "                    5S       1010         79      0.923       0.91      0.959      0.581\n",
      "                    6C       1010         68      0.917      0.882       0.96      0.645\n",
      "                    6D       1010         69       0.88      0.812      0.926      0.589\n",
      "                    6H       1010         83      0.978      0.843      0.971      0.651\n",
      "                    6S       1010         71      0.895      0.859      0.954      0.621\n",
      "                    7C       1010         53      0.981      0.991      0.994      0.681\n",
      "                    7D       1010         86      0.973      0.988      0.993      0.661\n",
      "                    7H       1010         75       0.96      0.962      0.992      0.672\n",
      "                    7S       1010         90      0.946      0.968      0.992      0.651\n",
      "                    8C       1010         65      0.904      0.865      0.955      0.641\n",
      "                    8D       1010        101      0.919      0.901       0.97      0.625\n",
      "                    8H       1010         86      0.875      0.895      0.954      0.653\n",
      "                    8S       1010         82      0.911      0.902      0.963      0.635\n",
      "                    9C       1010         61      0.894          1      0.992      0.632\n",
      "                    9D       1010         65       0.94      0.964      0.979      0.612\n",
      "                    9H       1010         77      0.966      0.987      0.993      0.658\n",
      "                    9S       1010         82      0.937      0.915      0.979      0.639\n",
      "                    AC       1010         69      0.928      0.935      0.972      0.642\n",
      "                    AD       1010         85      0.886      0.976      0.969      0.595\n",
      "                    AH       1010         68      0.937      0.971      0.983      0.651\n",
      "                    AS       1010         90      0.905      0.933      0.978      0.649\n",
      "                    JC       1010         69       0.92      0.942      0.983      0.577\n",
      "                    JD       1010         79      0.935      0.905      0.973       0.63\n",
      "                    JH       1010         61      0.906      0.948       0.96      0.559\n",
      "                    JS       1010         91      0.954      0.917      0.982      0.583\n",
      "                    KC       1010         68      0.941      0.941      0.982      0.588\n",
      "                    KD       1010         71      0.853      0.902      0.953      0.642\n",
      "                    KH       1010         81      0.913      0.765       0.96      0.645\n",
      "                    KS       1010         65      0.934      0.871      0.971      0.596\n",
      "                    QC       1010         75      0.923      0.987      0.983      0.644\n",
      "                    QD       1010        100      0.904       0.94      0.976      0.675\n",
      "                    QH       1010         81      0.937      0.914      0.958      0.551\n",
      "                    QS       1010         80      0.949      0.963      0.989      0.636\n",
      "Speed: 0.5ms preprocess, 4.4ms inference, 0.0ms loss, 2.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n",
      "üí° Learn more at https://docs.ultralytics.com/modes/val\n"
     ]
    }
   ],
   "source": [
    "!yolo val model=\"runs/detect/train_card_v3/weights/best.pt\" data=data2-test.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dc9471d-97f4-48bd-9619-5a87e0152f02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'38_Tennis_Tennis_38_580_jpg.rf.997d19d1ee3a6b384fd95be22255f637.jpg'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loc = \"datasets/WIDER-FACE-1/valid/images/\"\n",
    "val = os.listdir(val_loc)\n",
    "val[1005]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a66a5db7-2646-4828-b0f7-8a63b617ed24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 üöÄ Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "image 1/1 /home/jupyter/fd_widerface_yolov8/datasets/WIDER-FACE-1/valid/images/41_Swimming_Swimming_41_822_jpg.rf.f984914614bec7204f04f5fd110e1003.jpg: 640x640 8 persons, 1 bird, 1 banana, 13.4ms\n",
      "Speed: 3.1ms preprocess, 13.4ms inference, 547.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
      "üí° Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# Running on a sample image\n",
    "!yolo predict model=yolov8n.pt source=\"datasets/WIDER-FACE-1/valid/images/41_Swimming_Swimming_41_822_jpg.rf.f984914614bec7204f04f5fd110e1003.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee88afcd-e734-4d8d-a780-cfd2699c33d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running Prediction on Video Footage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3518073f-2d8f-4ba3-82ec-7d36f301394e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=ek05M8eCk7M\n",
      "[youtube] ek05M8eCk7M: Downloading webpage\n",
      "[youtube] ek05M8eCk7M: Downloading ios player API JSON\n",
      "[youtube] ek05M8eCk7M: Downloading android player API JSON\n",
      "[youtube] ek05M8eCk7M: Downloading m3u8 information\n",
      "[info] ek05M8eCk7M: Downloading 1 format(s): 22\n",
      "[download] Destination: datasets/youtube_vid/[ÌéòÏù¥Ïä§Ï∫†4K] Îâ¥ÏßÑÏä§ ÌïòÎãà 'Ditto' (NewJeans HANNI FaceCam) @SBS Inkigayo 230115.mp4\n",
      "[download] 100% of   39.58MiB in 00:00:02 at 15.41MiB/s  \n"
     ]
    }
   ],
   "source": [
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "# URLS = [\"https://www.youtube.com/watch?v=5F6j4e1C4Zk\"]  # Card\n",
    "# URLS = [\"https://www.youtube.com/watch?v=G_So72lFNIU\"]  # Card\n",
    "URLS = [\"https://www.youtube.com/watch?v=ek05M8eCk7M\"]  # Face\n",
    "\n",
    "\n",
    "dl_ops = {\n",
    "  'outtmpl': 'datasets/youtube_vid/%(title)s.%(ext)s'\n",
    "}\n",
    "\n",
    "with YoutubeDL(dl_ops) as ydl:\n",
    "    ydl.download(URLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739a2e1-2743-491b-a4da-fa064375007c",
   "metadata": {},
   "source": [
    "### Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8d7bfc84-828b-4462-b74e-adc535210bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "def video_to_frames(input_loc, output_loc, num_frames=10):\n",
    "    \"\"\"Function to extract frames from input video file\n",
    "    and save them as separate frames in an output directory.\n",
    "    Args:\n",
    "        input_loc: Input video file.\n",
    "        output_loc: Output directory to save the frames.\n",
    "        num_frames: Desired number of equally spaced frames.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(output_loc)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # Log the time\n",
    "    time_start = time.time()\n",
    "\n",
    "    # Start capturing the feed\n",
    "    cap = cv2.VideoCapture(input_loc)\n",
    "\n",
    "    # Find the number of frames\n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "    print(\"Number of available frames: \", video_length)\n",
    "    print(\"Number of printing frames: \", min(num_frames, video_length))\n",
    "\n",
    "    # Calculate the step size to get equally spaced frames\n",
    "    step_size = max(1, video_length // num_frames)\n",
    "\n",
    "    count = 0\n",
    "    print(\"Converting video..\\n\")\n",
    "\n",
    "    # Start converting the video\n",
    "    while cap.isOpened():\n",
    "        # Extract the frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Check if the current frame should be captured\n",
    "        if count % step_size == 0:\n",
    "            # Write the results back to the output location.\n",
    "            cv2.imwrite(output_loc + \"/%#05d.jpg\" % (count//step_size + 1), frame)\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "        # If there are no more frames left\n",
    "        if count > video_length - 1:\n",
    "            # Log the time again\n",
    "            time_end = time.time()\n",
    "            # Release the feed\n",
    "            cap.release()\n",
    "            # Print stats\n",
    "            print(\"Done extracting frames.\\n%d frames extracted\" % (count//step_size))\n",
    "            print(\"It took %d seconds for conversion.\" % (time_end - time_start))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d30c5ec3-def7-4ae1-8339-3202d8e0fc24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[ÌéòÏù¥Ïä§Ï∫†4K] Îâ¥ÏßÑÏä§ ÌïòÎãà 'Ditto' (NewJeans HANNI FaceCam) @SBS Inkigayo 230115.mp4\",\n",
       " 'ùíëùíêùíóÔºö vibing to your own song (feat. LAUV).mp4',\n",
       " 'How To Improve Your Juggling In Football ‚öΩ.mp4',\n",
       " 'Blackjack Expert Explains How Card Counting Works ÔΩú WIRED.mp4',\n",
       " 'How to Stack Playing Cards ÔΩú WIRED.mp4']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_loc = \"datasets/youtube_vid/\"\n",
    "video = os.listdir(video_loc)\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "937f4ce9-fde2-4fe3-8a3f-f4feb679e27e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available frames:  8318\n",
      "Number of printing frames:  20\n",
      "Converting video..\n",
      "\n",
      "Done extracting frames.\n",
      "20 frames extracted\n",
      "It took 23 seconds for conversion.\n"
     ]
    }
   ],
   "source": [
    "input_loc = video_loc + video[0]\n",
    "output_loc = \"datasets/youtube_frames\"\n",
    "\n",
    "video_to_frames(input_loc, output_loc,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7b29e6ee-174a-44b1-a875-e9fb4f02fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 üöÄ Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "Model summary (fused): 168 layers, 3015788 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/21 /home/jupyter/datasets/youtube_frames/00001.jpg: 384x640 (no detections), 64.5ms\n",
      "image 2/21 /home/jupyter/datasets/youtube_frames/00002.jpg: 384x640 (no detections), 11.8ms\n",
      "image 3/21 /home/jupyter/datasets/youtube_frames/00003.jpg: 384x640 (no detections), 7.1ms\n",
      "image 4/21 /home/jupyter/datasets/youtube_frames/00004.jpg: 384x640 (no detections), 6.8ms\n",
      "image 5/21 /home/jupyter/datasets/youtube_frames/00005.jpg: 384x640 (no detections), 6.9ms\n",
      "image 6/21 /home/jupyter/datasets/youtube_frames/00006.jpg: 384x640 (no detections), 6.7ms\n",
      "image 7/21 /home/jupyter/datasets/youtube_frames/00007.jpg: 384x640 (no detections), 6.8ms\n",
      "image 8/21 /home/jupyter/datasets/youtube_frames/00008.jpg: 384x640 (no detections), 10.1ms\n",
      "image 9/21 /home/jupyter/datasets/youtube_frames/00009.jpg: 384x640 (no detections), 10.1ms\n",
      "image 10/21 /home/jupyter/datasets/youtube_frames/00010.jpg: 384x640 (no detections), 6.8ms\n",
      "image 11/21 /home/jupyter/datasets/youtube_frames/00011.jpg: 384x640 (no detections), 6.9ms\n",
      "image 12/21 /home/jupyter/datasets/youtube_frames/00012.jpg: 384x640 (no detections), 7.0ms\n",
      "image 13/21 /home/jupyter/datasets/youtube_frames/00013.jpg: 384x640 (no detections), 6.8ms\n",
      "image 14/21 /home/jupyter/datasets/youtube_frames/00014.jpg: 384x640 (no detections), 6.9ms\n",
      "image 15/21 /home/jupyter/datasets/youtube_frames/00015.jpg: 384x640 (no detections), 11.3ms\n",
      "image 16/21 /home/jupyter/datasets/youtube_frames/00016.jpg: 384x640 (no detections), 8.8ms\n",
      "image 17/21 /home/jupyter/datasets/youtube_frames/00017.jpg: 384x640 1 AH, 6.6ms\n",
      "image 18/21 /home/jupyter/datasets/youtube_frames/00018.jpg: 384x640 (no detections), 8.0ms\n",
      "image 19/21 /home/jupyter/datasets/youtube_frames/00019.jpg: 384x640 (no detections), 6.9ms\n",
      "image 20/21 /home/jupyter/datasets/youtube_frames/00020.jpg: 384x640 (no detections), 6.9ms\n",
      "image 21/21 /home/jupyter/datasets/youtube_frames/00021.jpg: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.6ms preprocess, 10.5ms inference, 27.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict12\u001b[0m\n",
      "üí° Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "!yolo predict model=\"runs/detect/train_card_v3/weights/best.pt\" source='datasets/youtube_frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "90ff0ada-bfb5-4b81-bf60-c7c79d7e5075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 üöÄ Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "image 1/21 /home/jupyter/datasets/youtube_frames/00001.jpg: 384x640 4 persons, 1 train, 1 tie, 74.3ms\n",
      "image 2/21 /home/jupyter/datasets/youtube_frames/00002.jpg: 384x640 4 persons, 3 ties, 8.0ms\n",
      "image 3/21 /home/jupyter/datasets/youtube_frames/00003.jpg: 384x640 4 persons, 1 umbrella, 6.7ms\n",
      "image 4/21 /home/jupyter/datasets/youtube_frames/00004.jpg: 384x640 4 persons, 2 umbrellas, 7.4ms\n",
      "image 5/21 /home/jupyter/datasets/youtube_frames/00005.jpg: 384x640 3 persons, 7.6ms\n",
      "image 6/21 /home/jupyter/datasets/youtube_frames/00006.jpg: 384x640 2 persons, 1 tie, 11.3ms\n",
      "image 7/21 /home/jupyter/datasets/youtube_frames/00007.jpg: 384x640 2 persons, 10.9ms\n",
      "image 8/21 /home/jupyter/datasets/youtube_frames/00008.jpg: 384x640 2 persons, 13.0ms\n",
      "image 9/21 /home/jupyter/datasets/youtube_frames/00009.jpg: 384x640 2 persons, 1 tie, 13.1ms\n",
      "image 10/21 /home/jupyter/datasets/youtube_frames/00010.jpg: 384x640 1 person, 13.6ms\n",
      "image 11/21 /home/jupyter/datasets/youtube_frames/00011.jpg: 384x640 2 persons, 1 remote, 12.2ms\n",
      "image 12/21 /home/jupyter/datasets/youtube_frames/00012.jpg: 384x640 2 persons, 10.9ms\n",
      "image 13/21 /home/jupyter/datasets/youtube_frames/00013.jpg: 384x640 1 person, 1 clock, 7.3ms\n",
      "image 14/21 /home/jupyter/datasets/youtube_frames/00014.jpg: 384x640 1 person, 1 stop sign, 7.0ms\n",
      "image 15/21 /home/jupyter/datasets/youtube_frames/00015.jpg: 384x640 3 persons, 6.9ms\n",
      "image 16/21 /home/jupyter/datasets/youtube_frames/00016.jpg: 384x640 4 persons, 6.9ms\n",
      "image 17/21 /home/jupyter/datasets/youtube_frames/00017.jpg: 384x640 5 persons, 7.4ms\n",
      "image 18/21 /home/jupyter/datasets/youtube_frames/00018.jpg: 384x640 6 persons, 2 cell phones, 6.9ms\n",
      "image 19/21 /home/jupyter/datasets/youtube_frames/00019.jpg: 384x640 2 persons, 7.0ms\n",
      "image 20/21 /home/jupyter/datasets/youtube_frames/00020.jpg: 384x640 (no detections), 7.2ms\n",
      "image 21/21 /home/jupyter/datasets/youtube_frames/00021.jpg: 384x640 (no detections), 7.3ms\n",
      "Speed: 1.7ms preprocess, 12.0ms inference, 34.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "üí° Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "!yolo predict model=yolov8n.pt source='datasets/youtube_frames'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7f1771-8124-4692-aa78-36080fdc104d",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "You can see that many cards are not captured even though it is visible to the eye. \n",
    "\n",
    "Two options: \n",
    "1. Instead of fine tuning the last few layers of YOLOV8n, we can retrain the whole model: but this may not solve the issue of overfitting to training data that we have. Our predict data is from YouTube video and that is formatted differently from the train/test/val sample set.\n",
    "2. Train a card edge detection first before passing into a seperate final classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dadc19-0fa1-4c64-979f-325754f5a6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ab06d3c-29fe-4c4a-91c2-cf176642bdec",
   "metadata": {},
   "source": [
    "## Using Video Footage to Self-Generate More Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f28842-1827-459f-9a2f-c52747916af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-15:m116"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
